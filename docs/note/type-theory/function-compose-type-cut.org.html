<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>note/type-theory/function-compose-type-cut.org</title>
</head>
<body>
<pre style="white-space: pre-wrap;">#+title: function compose, type cut, and the algebra of logic

* intuitionistic logic

  - first I would like to summarize intuitionistic logic
    it is also called constructive logic
    it is very well known now
    and is gloriously called the Brouwer–Heyting–Kolmogorov interpretation

*** and

    - to prove (P and Q)
      is to prove P and prove Q
      - this is the same as classical logic

*** or

    - to prove (P or Q)
      is to prove P or prove Q
      - while in classical logic
        you can prove (P or Q)
        without a proof of P
        and without a proof of Q

*** imply

    - to prove (P -&gt; Q)
      is to prove that
      if we have a proof of P
      then we can construct a proof of Q
      - this is the same as classical logic

*** not

    - to prove (not P)
      is to prove (P -&gt; something-we-consider-wrong)
      - something-we-consider-wrong like (0 = 1)
      - (not (not P)) is weaker than P
        while in classical logic
        (not (not P)) is equal to P

*** for all

    - to prove &quot;for all x belong to A, we have P(x)&quot;
      is to prove that
      for all x belong to A
      we can construct a proof of P(x)
      - how to construct a proof discuss later
      - this is the same as classical logic

*** there exist

    - to prove &quot;there exist x belong to A, such that P(x)&quot;
      is to construct a value of type A
      and construct a proof of P(x)
      - the only way to prove something exist
        is to find such thing
      - while in classical logic
        to prove something exist
        you do not need to find such thing

* formal theorem

  - let us design a formal language to express theorem
    - [[remark on formalization]]

*** imply

    - firstly we see the fact that the general form of theorem is like (A -&gt; B)
      let us unite our syntax toward &quot;-&gt;&quot;
      we do not write A
      instead we will write (-&gt; A)
      - this is just like one does not write 3 but write 1\3 or 3/1 instead

*** and

    - and let us optimize our syntax for &quot;and&quot;
      we do not write ((A and B) -&gt; (C and D))
      but just write (A B -&gt; C D)

    - I call express of form (A B C ... -&gt; E F G ...) sequent or arrow
      this term is taken from Gentzen
      but you should note that
      sequent for us is not exactly the same as sequent for Gentzen
      for Gentzen (A B -&gt; C D) is viewed as ((A and B) -&gt; (C or D))
      but for us (A B -&gt; C D) is viewed as ((A and B) -&gt; (C and D))
      - if you want to know more about the meaning of sequent for Gentzen
        please see his paper &quot;investigations into logical deduction&quot;

*** or

    - for &quot;or&quot; we write (A or B) (A B or C D) (A or B or C) etc

*** not

    - I suggest to ignore &quot;not&quot;
      because you see that (not P) is just (P -&gt; something-we-consider-wrong)
      the negation we want to express is parameterized by something-we-consider-wrong
      (or say, depends on something-we-consider-wrong)
      if we simple say (not P)
      the information of this parameterization will be lost

*** for all &amp; there exist

    - I express &quot;for all x belong to A, we have P(x)&quot;
      as ((x : A) -&gt; x P)
      and express &quot;there exist x belong to A, such that P(x)&quot;
      as (-&gt; (x : A) x P)
      I am using postfix notation here
      I write &quot;x P&quot; instead of &quot;P(x)&quot;
      - you might argue that
        compare to the traditional math notation, postfix looks really alien
        if you care, please see my [[rationale of using postfix notation]]

    - recall that
      ((x : A) -&gt; x P) means &quot;for all x belong to A, we have P(x)&quot;
      (-&gt; (x : A) x P) means &quot;there exist x belong to A, such that P(x)&quot;
      in the above two example expressions
      variables are written in lower-case latin letter
      functions are written in upper-case latin letter
      personally I do not wish to
      distinguish meaning by lower-case v.s. upper-case
      (maybe because I am a scheme user, or maybe because I am a Chinese)
      so, in the following implementation of our language
      I will write
      #+begin_src scheme
      ((:x : a) -&gt; :x p)
      (-&gt; (:x : a) :x p)
      #+end_src

*** a detour

    - now we have designed a formal language to express theorem
      the best way to show how to use formal theorem in deduction
      is a detour through functional programming language
      theorem -&gt; type -&gt; function -&gt; proof

* theorem as type

  - theorem can be viewed as type of function
    this is well known as &quot;theorem as type&quot;

  - that means the language we just designed for formal theorem
    can also be used to describe the type of function

*** term-rewriting-system

    - our functional programming language will be a term-rewriting-system
      with postfix notation

    - examples about natural number
      #+begin_src scheme
      (+ natural (-&gt; type)
         zero (-&gt; natural)
         succ (natural -&gt; natural))

      (~ add (natural natural -&gt; natural)
         (:m zero -&gt; :m)
         (:m :n succ -&gt; :m :n add succ))
      #+end_src

    - in the above example
      &quot;+&quot; can be readed as &quot;define-type&quot;
      the type to be defined is &quot;natural&quot;
      and the type of &quot;natural&quot; is (-&gt; type)
      &quot;natural&quot; has two data-constructors
      they are &quot;zero&quot; with type (-&gt; natural)
      and &quot;succ&quot; with type (natural -&gt; natural)
      - each of them has a type
        means each of them can be viewed as a function
        although they have no function body
        because when viewed as function, they are trivial
      - they are actually not only trivial
        but also reversible
        this means they can be used as pattern in pattern-matching
        because every reversible function
        can be used as pattern in pattern-matching
        (although I am not able to achieve it in this language)

    - in the above example
      &quot;~&quot; can be readed as &quot;define-function&quot;
      - in other places, &quot;~&quot; can also be readed as &quot;define-theorem&quot;
      the function to be defined is &quot;add&quot;
      the type of &quot;add&quot; is (natural natural -&gt; natural)
      and its function body has two arrows
      first (:m zero -&gt; :m)
      second (:m :n succ -&gt; :m :n add succ)
      - I will call the left part of an arrow &quot;antecedent&quot;
        and the right part of an arrow &quot;succedent&quot;
        so we have
        ( &quot;antecedent&quot; -&gt; &quot;succedent&quot; )

    - the semantic of function
      can be explained by explaining
      what happens when we apply a function to its arguments

    - when applying a function
      the interperter will try to match (or cover) its arguments
      with the antecedent of each arrow in the function body
      - for &quot;add&quot;, the antecedents will be (:m zero) and (:m :n zero)
      when one antecedent successes
      it will bind variables occurs in the antecedent to data in arguments
      and rewrite its corresponding succedent
      and returned the result
      - because all functions are &quot;cover-checked&quot;
        there must be at least one antecedent can cover the arguments
        then there are many antecedents can cover the arguments
        the first one is used
      - note that
        the order of rewriting is revealed by the postfix notation
      - &quot;use the first covering antecedent&quot; and &quot;the order of rewriting&quot;
        conclude the reduction strategy of our term-rewriting-system

*** stack processing functions

    - beside function application
      we can also use function composition to explain the semantic of function
      (because we are using postfix notation)

    - and with the help of a stack
      we will be able to handle function composition of functions
      with multiple arguments and multiple return values

    - for example, we can define the following stack processing functions
      #+begin_src scheme
      (~ drop (:t -&gt;)
         (:d -&gt;))

      (~ dup (:t -&gt; :t :t)
         (:d -&gt; :d :d))

      (~ over (:t1 :t2 -&gt; :t1 :t2 :t1)
         (:d1 :d2 -&gt; :d1 :d2 :d1))

      (~ tuck (:t1 :t2 -&gt; :t2 :t1 :t2)
         (:d1 :d2 -&gt; :d2 :d1 :d2))

      (~ swap (:t1 :t2 -&gt; :t2 :t1)
         (:d1 :d2 -&gt; :d2 :d1))
      #+end_src

    - [[remark on the use of stack in implementation]]

*** more examples

    - more examples about natural number
      #+begin_src scheme
      (~ mul (natural natural -&gt; natural)
         (:m zero -&gt; zero)
         (:m :n succ -&gt; :m :n mul :m add))

      (~ factorial (natural -&gt; natural)
         (zero -&gt; zero succ)
         (:n succ -&gt; :n factorial :n succ mul))
      #+end_src

    - examples about list
      #+begin_src scheme
      (+ list (type -&gt; type)
         null (-&gt; :t list)
         cons (:t list :t -&gt; :t list))

      (~ append (:t list :t list -&gt; :t list)
         (:l null -&gt; :l)
         (:l :r :e cons -&gt; :l :r append :e cons))

      (~ map (:t1 list (:t1 -&gt; :t2) -&gt; :t2 list)
         (null :f -&gt; null)
         (:l :e cons :f -&gt; :l :f map :e :f apply cons))
      #+end_src

    - examples about vector
      function bodys are the same as examples about list
      but the types also express the information about the length of list
      #+begin_src scheme
      (+ vector (natural type -&gt; type)
         null (-&gt; zero :t vector)
         cons (:n :t vector :t -&gt; :n succ :t vector))

      (~ append (:m :t vector :n :t vector -&gt; :m :n add :t vector)
         (:l null -&gt; :l)
         (:l :r :e cons -&gt; :l :r append :e cons))

      (~ map (:n :t1 vector (:t1 -&gt; :t2) -&gt; :n :t2 vector)
         (null :f -&gt; null)
         (:l :e cons :f -&gt; :l :f map :e :f apply cons))
      #+end_src

    - more examples
      #+begin_src scheme
      &gt;&lt;
      #+end_src

* function as proof

  - now we are at the finial step of the detour from formal theorem to proof
    theorem -&gt; type -&gt; function -&gt; proof

  - this is well known as &quot;function as proof&quot;
    it says, the way we write function body forms a language to record deduction
    - [[remark on deduction and inference]]

  - a record of many steps of deduction is called a proof

  - the next question is
    when we use this kind of syntax to write function body
    what actions upon types we are recording?
    (what deduction rules we are recording?)

*** concatenation, composition and cut

    - first syntax operation is concatenation
      concatenation of two names corresponds to
      1. composition of two functions under these names
      2. cut of two types under these names

    - [[rationale of composition over application]]

    - by &quot;cut&quot; I mean the hero deduction rule
      which occupys the center of the stage of Gentzen&apos;s sequent calculus
      it says if we have (A -&gt; B) and (B -&gt; C)
      cut them together, we get (A -&gt; C)

    - on the other hand
      if we have function f1 of type (A -&gt; B) and f2 of (B -&gt; C)
      compose f1 and f2, we get a function of type (A -&gt; C)
      this is what I mean by &quot;function compose, type cut&quot; in the title

    - in the following example
      &quot;*&quot; can be readed as &quot;define-hypothesis&quot;
      #+begin_src scheme
      (* wanderer/poe (-&gt; poe is-wanderer))
      (* way-worn (:x is-wanderer -&gt; :x is-weary))

      (~ weary/poe (-&gt; poe is-weary)
         (-&gt; wanderer/poe way-worn))
      #+end_src

    - when view them as functions and types
      it is really intuitive to see
      with two functions &quot;wanderer/poe&quot; and &quot;way-worn&quot;
      how we can compose a function of type (-&gt; poe is-weary)
      this is why I said that
      the best way to show how to use formal theorems in deduction
      is a detour through functional programming language

*** other deduction rules of natural deduction

    - the following seems like conj-intro and conj-elim in natural deduction
      we can simply use stack processing function to express them
      - the types of stack processing functions
        should remind you of the so called structural rules of sequent calculus
      - linear logic and other substructural logics can be investigated under this framework
      #+begin_src scheme
      ;; conj-intro
      (* p1 (-&gt; a))
      (* p2 (-&gt; b))
      (~ p3 (-&gt; a b)
         (-&gt; p1 p2))

      (* drop (:t -&gt;)
         (:d -&gt;))
      (~ swap (:t1 :t2 -&gt; :t2 :t1)
         (:d1 :d2 -&gt; :d2 :d1))

      ;; conj-elim
      (* p3 (-&gt; a b))
      (~ p1 (-&gt; a)
         (-&gt; p3 drop))
      (~ p2 (-&gt; b)
         (-&gt; p3 swap drop))
      #+end_src

*** the meaning of proof

    - we have the advantage to observe
      the concrete meaning of &quot;proof&quot; within our concrete model

    - concretely, how proof (type) is checked by the language?
      I have the following summarization

      | arrow list in function body |                            |
      |-----------------------------+----------------------------|
      | for each arrow              | type-check                 |
      | for all antecedents         | cover-check                |
      | for each succedent          | structural-recursion-check |

    - to type-check one arrow, is to
      - unify the antecedent of type-arrow
        with the type of the antecedent of arrow
      - during which, variables will be bound to data or other variables
      - under these bindings
        try to cover the succedent of type-arrow
        by the type of the succedent of arrow

    - let us follow a check step by step
      #+begin_src scheme
      (+ natural (-&gt; type)
         zero (-&gt; natural)
         succ (natural -&gt; natural))

      (+ list (type -&gt; type)
         null (-&gt; :t list)
         cons (:t list :t -&gt; :t list))

      (~ map (:t1 list (:t1 -&gt; :t2) -&gt; :t2 list)
         (null :f -&gt; null)
         (:l :e cons :f -&gt; :l :f map :e :f apply cons))

      (+ has-length (:t list natural -&gt; type)
         null/has-length (-&gt; null zero has-length)
         cons/has-length (:l :n has-length -&gt; :l :a cons :n succ has-length))

      (~ map/has-length (:l :n has-length -&gt; :l :f map :n has-length)
         (null/has-length -&gt; null/has-length)
         (:h cons/has-length -&gt; :h map/has-length cons/has-length))

      ;; take the type check of the second arrow of map/has-length for example

      ;; unify the antecedent of type-arrow :
      (:l :n has-length)

      ;; with the type of antecedent of the second arrow :
      type of (:h cons/has-length)
      ==
      (:l:0 :a:0 cons :n:0 succ has-length)

      ;; bindings :
      ((:h : :l:0 :n:0 has-length)
       (:l = :l:0 :a:0 cons)
       (:n = :n:0 succ))

      ;; the type of the succedent of the second arrow :
      type of (:h map/has-length cons/has-length)
      == ;; under bindings
      (:l:0
       :n:0 has-length
       (type/apply map/has-length)
       (type/apply cons/has-length))
      ==
      (:l:0 :f:1 map
       :n:0 has-length
       (type/apply cons/has-length))
      ==
      (:l:0 :f:1 map :a:2 cons
       :n:0 succ has-length)

      ;; cover the succedent of type-arrow :
      (:l :f map :n has-length)
      == ;; under bindings
      (:l:0 :a:0 cons :f map
       :n:0 succ has-length)
      == ;; rewrite map
      (:l:0 :f map :a:0 :f apply cons
       :n:0 succ has-length)

      ;; cover :
      ((:f:1 = :f)
       (:a:2 = :a:0 :f apply))
      #+end_src

    - to summarize the meaning of &quot;proof&quot; within our concrete model
      - we can express theorems about
        - recursively defined data
        - recursively defined function
      - we can do proof by
        - cut -- function composition
        - exhaustion -- cover-check
        - structural induction --
          where first we proof some basic steps
          and by unification we get next-theorem
          (just as the next-number in natural-induction)
          a function recursive call is a use of the induction hypothesis
          aimming to prove the next-theorem

*** the meaning of proof, again

    - if we define natural number as the following
      then we can proof natural-induction
      #+begin_src scheme
      (+ natural (-&gt; type)
         zero (-&gt; natural)
         succ (natural -&gt; natural))

      (~ natural-induction ((:p : (natural -&gt; type))
                            zero :p apply
                            ((:k : natural) :k :p apply -&gt; :k succ :p apply)
                            (:x : natural) -&gt; :x :p apply)
         (:q :q/z :q/s zero -&gt; :q/z)
         (:q :q/z :q/s :n succ -&gt;
             :n
             :q :q/z :q/s :n natural-induction
             :q/s apply))

      ;; take the type check of the second arrow for example

      ;; unify the antecedent of type-arrow :
      ((:p : (natural -&gt; type))
       zero :p apply
       ((:k : natural) :k :p apply -&gt; :k succ :p apply)
       (:x : natural))

      ;; with the type of antecedent of the second arrow :
      type of (:q :q/z :q/s :n succ)

      ;; bindings :
      ((:p = :q)
       (:q : (natural -&gt; type))
       (:q/z : zero :p apply)
       (:q/s : ((:k : natural) :k :p apply -&gt; :k succ :p apply))
       (:x = :n)
       (:n : natural))

      ;; the type of the succedent of the second arrow :
      type of
      (:n
       :q :q/z :q/s :n natural-induction
       :q/s apply)
      == ;; under bindings
      ((:n : natural)
       (:q : (natural -&gt; type))
       (:q/z : zero :q apply)
       (:q/s : ((:k : natural) :k :q apply -&gt; :k succ :q apply))
       (:n : natural)
       natural-induction
       :q/s type/apply)
      ==
      ((:n : natural)
       :n :q apply
       :q/s type/apply)
      ==
      ((:n succ :q apply))

      ;; cover the succedent of type-arrow :
      (:x :p apply)
      == ;; under bindings
      ((:n succ :q apply))
      #+end_src

*** the use of &quot;or&quot;

    - when &quot;or&quot; is used
      we just need to cover all the cases
      #+begin_src scheme
      (~ length (:t list -&gt; natural)
         (null -&gt; zero)
         (:l :e cons -&gt; :l length succ))

      (~ length ((natural or :t list) -&gt; natural)
         (null -&gt; zero)
         (:l :e cons -&gt; :l length succ)
         (zero -&gt; zero)
         (:n succ -&gt; :n succ))

      (~ length ((natural or :t list) -&gt; natural)
         (null -&gt; zero)
         (:l :e cons -&gt; :l length succ)
         (:n -&gt; :n))
      #+end_src

    - type definition is like named &quot;or&quot;
      #+begin_src scheme
      (+ nali (-&gt; type)
         na (natural -&gt; nali)
         li (:t list -&gt; nali))

      (~ nali/length (nali -&gt; natural)
         (:l li -&gt; :l length)
         (:n na -&gt; :n))
      #+end_src

    - type definition is like named &quot;or&quot; of &quot;and&quot;s
      #+begin_src scheme
      (+ nanalili (-&gt; type)
         nana (natural natural -&gt; nanalili)
         lili (:t1 list :t2 list -&gt; nanalili))

      (~ nanalili/length (nanalili -&gt; natural)
         (:l1 li :l2 li -&gt; :l1 length :l2 length add)
         (:n1 na :n2 na -&gt; :n1 :n2 add))
      #+end_src

    - thus
      | function body                | deduction               |
      |------------------------------+-------------------------|
      | branching by a list of arrow | disj-elim or exist-elim |
      | binding by unification       | conj-elim               |

* algebra of logic

  - since function composition satisfy associative law
    I think I can design (or seek for) an algebraic structure
    for formal theorems

  - we will only define those algebraic operations
    that are closed in the set of derivable theorems

  - hopefully we will be able to capture all deduction by algebraic operations

  - [[remark on algebraic structure]]

*** to mimic fraction of natural number

    - let us view theorem (A -&gt; B) as fraction
      A as denominator
      B as numerator
      - so, one might write (A \ B)
        note that
        we are using reverse-slash instead of slash
        to maintain the order of A B in (A -&gt; B)

*** multiplication

    - to multiply two theorems (A -&gt; B) and (C -&gt; D)
      we get (A C -&gt; B D)
      - just like (A \ B) (C \ D) = (A C \ B D)

      #+begin_src scheme
      (* r (A -&gt; B))
      (* s (C -&gt; D))

      (~ r/s/mul (A C -&gt; B D)
         (:x :y -&gt; :x r :y s))

      ;; abstract it to a combinator
      (~ general/mul ((:a -&gt; :b) (:c -&gt; :d) -&gt; (:a :c -&gt; :b :d))
         (:r :s -&gt; (lambda (:a :c -&gt; :b :d)
                     (:x :y -&gt; :x :r apply :y :s apply))))
      #+end_src

    - theorems under multiplication is an Abelian group
      identity element is (-&gt;)
      inverse of (A -&gt; B) is (B -&gt; A)

*** two definitions of addition

***** first definition

      - this definition recalls the fraction of natural number
        but it seems not natural when written as function in our language

      - to add two theorems (A -&gt; B) and (C -&gt; D)
        we get (A B -&gt; (B C or A D))
        - just like (A \ B) + (C \ D) = (A C \ (B C + A D))

        #+begin_src scheme
        (* r (A -&gt; B))
        (* s (C -&gt; D))

        (~ r/s/fraction-add (A C -&gt; (B C or A D))
           (:x :y -&gt; :x r :y)
           (:x :y -&gt; :x :y s))

        ;; abstract it to a combinator
        (~ general/fraction-add ((:a -&gt; :b) (:c -&gt; :d) -&gt; (:a :c -&gt; (:b :c or :a :d)))
           (:r :s -&gt; (lambda (:a :c -&gt; (:b :c or :a :d))
                       (:x :y -&gt; :x :r apply :y)
                       (:x :y -&gt; :x :y :s apply))))
        #+end_src

      - distributive is just like fraction of natural number
        because the way we define addition
        is just like the addition of fraction of natural number

      - theorems under addition is an Abelian semigroup
        we do not have identity element
        and we do not have inverse
        - of course, we can introduce a &quot;zero-theorem&quot;
          (a theorem that we can never prove)
          as the identity element of addition
          to make our algebraic structure more like fraction of natural number
          but let us do not do this for now

      - under this definition of addition
        one may call the algebraic structure &quot;natural field&quot;
        to recall its similarites between the fraction of natural number
        - note that
          other terms like &apos;semi-field&apos; is ambiguous
          because it does not inform us
          whether addition or multiplication is semi

***** second definition

      - this definition seems natural in our language

      - to add two theorems (A -&gt; B) and (C -&gt; D)
        we get ((A or B) -&gt; (C or D))

        #+begin_src scheme
        (* r (A -&gt; B))
        (* s (C -&gt; D))

        (~ r/s/mul-like-add ((A or C) -&gt; (B or D))
           (:x -&gt; :x r)
           (:y -&gt; :y s))

        ;; abstract it to a combinator
        (~ general/mul-like-add ((:a -&gt; :b) (:c -&gt; :d) -&gt; ((:a or :c) -&gt; (:b or :d)))
           (:r :s -&gt; (lambda ((:a or :c) -&gt; (:b or :d))
                       (:x -&gt; :x :r apply)
                       (:y -&gt; :y :s apply))))
        #+end_src

      - distributive also hold under this definition of addition
        because (-&gt; A (B or C)) is the same as (-&gt; (A B or A C))

      - theorems under addition is an Abelian semigroup
        identity element is (-&gt;)
        but we do not have inverse

*** term-lattice, and cut as weaken

    - this is where we must take term-lattice into account

      | lattice          | term                   |
      |------------------+------------------------|
      | meet             | unification (uni)      |
      | join             | anti-unification (ani) |
      | greater-or-equal | cover (or match)       |

      - note that
        equal can be defined by greater-or-equal

    - term-lattice is also called &quot;subsumption lattice&quot; by other authers
      I call it &quot;term-lattice&quot;
      because I want to make explicit its relation with term-rewriting-system
      (I will address the detail of term-lattice in another paper)

    - if we have (A -&gt; B) and (C -&gt; D)
      we can cut them only when (C cover B)
      for example when
      - C = B
      - C = (B or E)
      - C = :x :y P
        B = :x :x P

    - cut can be viewed as an important way to weaken a theorem
      because we can first
      multiply (A -&gt; B) and (C -&gt; D)
      to (A C -&gt; B D)
      then weaken it to (A -&gt; D)
      - provides that (C cover B)

    - we can extend the term-lattice to cedent (antecedent and succedent)
      because cedent is Cartesian product of term in the term-lattice

*** type-check, again

    - with the new terminology introduced by term-lattice
      we can express type-check in a better way

    - type-arrow : (A -&gt; B)
      arrows in function body : (a1 -&gt; b1) (a2 -&gt; b2)
      (A uni a1) ((rewrite b1) cover (rewrite B))
      (A uni a2) ((rewrite b1) cover (rewrite B))
      - note that
        after (A uni a1)
        ((rewrite b1) cover (rewrite B)) is performed with new bindings

*** &gt;&lt; definition as extension of algebraic structure

    - &gt;&lt;

    - extend freely by &quot;*&quot;

    - extend by &quot;+&quot;

    - difference between &quot;+&quot; and &quot;*&quot;

* &gt;&lt; I do not know

  - &gt;&lt;

* implementation

  - I made an attempt to implement a prototype of the language
    (project page at <a href="http://xieyuheng.github.io/sequent1)">http://xieyuheng.github.io/sequent1)</a>

*** &gt;&lt; limits of my implementation

* appendixes

*** remark on formalization

    - I agree with Errett Bishop who said
      &quot;a proof is any completely convincing argument.&quot;
      I also think theorems expressed by formal language are specially clear
      and proofs checked by computer are specially convincing

    - on the other hand
      I also think that
      formal language can never be used to satisfactorily explain
      or totally simulate human language
      formal theorem and formal proof can never fully capture &quot;human proof&quot;
      this fact is specially clear
      if you are willing to think of &quot;human proof&quot; historically

    - the aim (or one aim) of formalization is to reduce (or remove) vagueness
      while the definition of vagueness is always vague

*** rationale of using postfix notation

    - rationale of using postfix notation is the following
      in the linear writing system of our language
      we can roughly distinguish four kinds of notations for function or predicate
      | infix     | ((1 + 2) + 3) |
      | prefix    | + + 1 2 3     |
      | postfix   | 3 2 1 + +     |
      | borderfix | (+ 1 2 3)     |
      - infix is especially good for associative binary function
      - prefix and postfix are not ambiguous without bracket
      - borderfix can be used for functions
        that can apply to different numbers of arguments
      our choice is between prefix and postfix
      because for simplicity we have the following two features
      - the arity of all functions must be fixed
      - we want our expressions to be not ambiguous without bracket
      then, how do we decide to use postfix instead of prefix?
      seemingly, prefix and postfix are symmetric
      while we still can distinguish them
      because we write in special order (from left to right in most western language)
      in postfix notation suppose we have written
      1 2 +
      and we want to add 3 to the result of 1 2 +
      we simply write
      1 2 + 3 +
      while in prefix notation suppose we have written
      @@html: + 1 2 @@
      and we want to add 3 to the result of + 1 2
      we have to insert + 3 in front of + 1 2 and write
      @@html: + 3 + 1 2 @@
      I summarize this difference by say
      postfix notation respect the special order of a linear writing system
      the above conclude my rationale

*** remark on the use of stack in implementation

    - first few versions is implemented as a stack-based language
      only later, changed to term-rewriting-system
      to make type inference easier

    - for basic information about stack-based language
      please see forth (the language)

    - for enlightening view of stack
      please see joy (the language)

*** rationale of composition over application

    - to optimize system for composition
      is to denote composition by concatenation of term

    - when optimize syntax for composition instead of application
      - we get better algebra-like structure
        because function composition is associative
        while function application is not
      - we lost good syntax for currying
        because currying is designed as a convention
        of the syntax of function application

*** remark on deduction and inference

    - one might ask, what is a deduction or a inference?
      my answer is a deduction or a inference
      is a way to express a change of theorem
      &quot;a change&quot; means &quot;one step of change&quot;

    - let us generalize it a little bit
      and to discuss &quot;a change of thing&quot; and &quot;language to record changes&quot;
      you will find these two concepts are very common
      and they also are named differently in different places
      | thing   | a change of thing     | language to record changes |
      |---------+-----------------------+----------------------------|
      | theorem | deduction             | proof                      |
      | food    |                       | cookbook                   |
      | data    |                       | algorithm                  |
      | number  | elementary arithmetic |                            |
      (seems to me like a market for language designer)

*** &gt;&lt; remark on algebraic structure
</pre>
</body>
</html>
