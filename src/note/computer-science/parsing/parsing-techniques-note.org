#+title: parsing techniques note

- parsing techniques - A Practical Guide
  - Second Edition
  - by Dick Grune and Ceriel J.H. Jacobs

* [note]

*** rich mathematical structure of parser theory

    - the mathematical structure of grammar is very rich
      and can be generalized to higher dimension

    - parser theory and algebraic topology :
      - translation between formal-grammar and automaton

*** parser theory as template theory

    - parser theory can be viewed as a template theory,
      where basic language for discussion about it
      is alreay well established.
      and how the language is established can be copied
      as a template, to study other problem domains.

* 1 intro

  - parsing is the process of structuring a linear representation
    in accordance with a given grammar.

    - x -
      the above statement
      seems is the reverse of the action of a parser.
      because a parser construct AST from linear representation.
      but actually, if given grammar,
      we can generate the linear pattern
      and match it against the concrete linear representation,
      then we can get the AST,
      because the grammar is the type of a AST node,
      and the matched pattern give us the body of the node.

  - The reverse problem -- given a (large) set of sentences,
    find the/a grammar which produces them
    -- is called grammatical inference.

    - https://en.wikipedia.org/wiki/Grammar_induction

    - x -
      this seems is the true art.

* 2 grammars as a generating device

*** grammar describe language

    - language -- set of sentences
      describe -- constructive generating
      grammar -- generative grammar

*** can all languages be described by finite descriptions?

    - The dis-proof of
      > Can all languages be described by finite descriptions?
      by diagonalization, is wrong.

    - For descriptions can not be Enumerated,
      because for a specific description,
      there alreay are infinitely many ways to interpret
      how the specific description describe a language.

*** 2.1.4 Describing a Language through a Finite Recipe

    #+begin_src rust
    example_sentences! {
        "tom";
        "tom and dick";
        "tom, dick and harry";
    }

    grammar! {
        Name = "tom";
        Name = "dick";
        Name = "harry";

        Sentence = Name;
        Sentence = List End;

        List = Name;
        List = Name ", " List;

        ", " Name End = " and " Name;
    }
    #+end_src

*** 2.2.1 The Formalism of Formal Grammars

    - symbols
      - non-terminal symbols
      - terminal symbols

      two type of symbols must be mutual exclusive

    - rules
      a rule can rewrite symbol-pattern to symbol-pattern
      where left-hand side must be non-empty
      while right-hand side might be empty

    - a non-terminal symbol as starting-symbol

      from the starting-symbol,
      we generate symbol-patterns by rules.

      the result can be viewed as a sentence in the language,
      if it contains only terminal symbols

*** 2.2.2 Generating Sentences from a Formal Grammar

    - this kinds of formal-grammar
      is called phrase structure grammar [PS].

    #+begin_src rust
    grammar! {
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List End;
        List = Name | Name ", " List;
        ", " Name End = " and " Name;
    }

    production! {
        Sentence;
        List End;
        Name ", " List End;
        Name ", " Name ", " List End;
        Name ", " Name End;
        Name ", " Name " and " Name;
        "tom, dick and harry";
    }
    #+end_src

*** 2.2.3 The Expressive Power of Formal Grammars

    - any set that can be generated by a program
      can be generated by a phrase structure grammar.

    #+begin_src rust
    grammar! {
        Start = Moves;
        // circle-movements-for-manhattan-turtle
        Moves = "north " Moves "south " | "east " Moves "west " | ε;
        ε = "";
        // the following are just
        //   swapping pathes generated by the above rule
        "north east " = "east north ";
        "north south " = "south north ";
        "north west " = "west north ";
        "east north " = "north east ";
        "east south " = "south east ";
        "east west " = "west east ";
        "south north " = "north south ";
        "south east " = "east south ";
        "south west " = "west south ";
        "west north " = "north west ";
        "west east " = "east west ";
        "west south " = "south west ";
    }
    #+end_src

*** 2.3 The Chomsky Hierarchy of Grammars and Languages

    - restrictions over formal-grammar,
      for simpler parsing algorithm.
      type-0 type-1 type-2 type-3

    - type-0 is unrestricted formal-grammar.
      no general parsing algorithm for them can exist,
      and all known special parsing algorithms
      are either very inefficient or very complex.

*** 2.3.1 Type 1 Grammars

    - A grammar is Type 1 monotonic
      if it contains no rules in which
      the left-hand side consists of more symbols
      than the right-hand side.

      This forbids, for example, the rule
      -- , N E = and N

    - A grammar is Type 1 context-sensitive [CS]
      if all of its rules are context-sensitive.

      A rule is context-sensitive
      if actually only one (non-terminal) symbol
      in its left-hand side
      gets replaced by other symbols,
      while we find the others back,
      undamaged and in the same order,
      in the right-hand side.

      Example:
      -- Name Comma Name End = Name and Name End
      which tells that the rule `-- Comma = and` may be applied
      if the left context is `Name`
      and the right context is `Name End`.
      The contexts themselves are not affected.

    #+begin_src rust
    grammar! {
        // type-0
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List End;
        List = Name | Name ", " List;
        ", " Name End = " and " Name;
    }

    grammar! {
        // type-1-monotonic-grammar
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List;
        List = EndName | Name ", " List;
        ", " EndName = " and " Name;
    }

    grammar! {
        // type-1-context-sensitive-grammar
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List;
        List = EndName | Name Comma List;
        Comma EndName = " and " EndName;
        " and " EndName = " and " Name;
        Comma = ", ";
    }
    #+end_src

*** 2.3.1.2 Constructing a Type 1 Grammar

    - The standard example of a Type 1 language
      is the set of words that consist of
      equal numbers of as, bs and cs, in that order:
      [a a a b b b c c c]

    #+begin_src rust
    grammar! {
        // type-1-monotonic-grammar
        S = "abc" | "a" S Q;
        "b" Q "c" = "bbcc";
        "c" Q = Q "c";
    }

    production! {
        S;
        "a" S Q;
        "aa" S Q Q;
        "aaabc" Q Q;
        "aaab" Q "c" Q;
        "aaabbcc" Q;
        "aaabb" Q "cc";
        "aaabbbccc";
    }
    #+end_src

*** 2.3.2 Type 2 Grammars

    - A context-free grammar [CF]
      is like a context-sensitive grammar,
      except that both the left and the right contexts
      are required to be absent (empty).

      As a result, the grammar may contain only rules that
      have a single non-terminal on their left-hand side.

    #+begin_src rust
    grammar! {
        // type-1-context-sensitive-grammar
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List;
        List = EndName | Name Comma List;
        Comma EndName = " and " EndName;
        " and " EndName = " and " Name;
        Comma = " ,";
    }

    grammar! {
        // type-2-context-free-grammar
        Name = "tom" | "dick" | "harry";
        Sentence = Name | List " and " Name;
        List = Name | Name ", " List;
    }
    #+end_src

*** 2.3.2.1 Production Independence

    - production process is simplified to production tree.

    - we do not need a non-terminal symbol as starting-symbol,
      every non-terminal can be viewed as a set.

    #+begin_src rust
    production! {
        Sentence;
        List " and " Name;
        Name ", " List " and " Name;
        Name ", " Name " and " Name;
        "tom, dick and harry";
    }
    #+end_src

*** 2.3.3 Type 3 Grammars

    - Type 2 grammars disallow context
      Type 3 grammars disallow nesting

    - for Type 3 grammars
      a right-hand side may only contain one non-terminal
      and it must come at the end.

      This means that there are only two kinds of rules:
      - a non-terminal produces zero or more terminals
      - a non-terminal produces zero or more terminals
        followed by one non-terminal.

    - The original Chomsky definition of Type 3
      restricts the kinds of rules to
      - a non-terminal produces one terminal
      - a non-terminal produces one terminal
        followed by one non-terminal.

    - Our definition is equivalent and more convenient,
      although the conversion to Chomsky Type 3
      is not completely trivial.

    - Type 3 grammars are also called regular grammars [RE]
      or finite-state grammars [FS]

      - right-regular-grammar -- the default regular-grammar
        the only non-terminal in a rule
        is found at the right end of the right-hand side

      - left-regular-grammar
        the only non-terminal in a rule
        is found at the left end of the right-hand side

    - Since regular grammars are used very often
      to describe the structure of text
      on the character level,
      it is customary for the terminal symbols of a regular grammar
      to be single characters.

    #+begin_src rust
    grammar! {
        // regular-grammar
        Sentences = "t" | "d" | "h" | List;
        List = "t" ListTail | "d" ListTail | "h" ListTail;
        ListTail = "," List | "&t" | "&d" | "&h";
    }

    grammar! {
        // left-regular-grammar
        Sentences = "t" | "d" | "h" | List;
        List = ListHead "&t" | ListHead "&d" | ListHead "&h";
        ListHead = ListHead ",t" | ListHead ",d" | ListHead ",h"
            | "t" | "d" | "h";
    }

    // a production-tree degenerates into
    // a production-chain of non-terminals
    // that drop a sequence of terminals on their left.

    production! {
        Sentences;
        List;
        "t" ListTail;
        "t," List;
        "t,d" ListTail;
        "t,d&h";
    }

    // [tdh] as abbreviation for t|d|h

    grammar! {
        // regular-grammar
        S = ["tdh"] | L;
        L = ["tdh"] T;
        T = "," L | "&" ["tdh"];
    }

    grammar! {
        // regular-grammar
        S = ["tdh"] | L;
        L = ["tdh"] "," L | "&" ["tdh"];
    }

    grammar! {
        // regular-chomsky-grammar
        S = ["tdh"] | ["tdh"] M;
        M = "," N | "&" P;
        N = ["tdh"] M;
        P = ["tdh"];
    }
    #+end_src

*** linear-grammar

    - There is a natural in-between class, Type 2.5 so to speak,
      in which only a single non-terminal
      is allowed in a right-hand side,
      but where it need not be at the end.
      This gives us the so-called linear-grammars.

*** regular expression

    - all regular grammars can be expressed by regular expression
      -- regular expression uese less names than regular grammar

    - regular expression operators
      - * -- zero-or-more
      - + -- one-or-more
      - ? -- zero-or-one
      - () -- group
      - [] -- choice

    #+begin_src rust
    S = ((["tdh"]",")* ["tdh"]"&")? ["tdh"];

    // NOTE if not at character level
    S = ((["t" "d" "h"] ",")* ["t" "d" "h"] "&")? ["t" "d" "h"];
    #+end_src

*** 2.3.4 Type 4 Grammars

    - no non-terminal is allowed in the right-hand side.

      This removes all the generative power from the mechanism,
      except for the choosing of alternatives.

    - finite-choice-grammar [FC]

    #+begin_src rust
    S = ["tdh"]
        | ["tdh"] "&" ["tdh"]
        | ["tdh"] "," ["tdh"] "&" ["tdh"];
    #+end_src

*** table of terminology

    | formal-grammar         | abbreviation | type |
    |------------------------+--------------+------|
    | phrase structure       | PS           |    0 |
    | context sensitive      | CS           |    1 |
    | monotonic              |              |    1 |
    | context free           | CF           |    2 |
    | linear                 |              |  2.5 |
    | finite-state (regular) | FS (RE)      |    3 |
    | finite-choice          | FC           |    4 |

* 3 Introduction to Parsing

*** intro

    - To parse a string according to a grammar
      means to reconstruct the production tree (or trees)
      that indicate how the given string
      can be produced from the given grammar.

      - inverses of phrase structure generators
        but actually, the study is limited to context free grammar
        because for phrase structure grammar
        the production process produce graph instead of tree

      - x -
        - the grammar rules in a context free grammar
          can be viewed as named by its left hand side
        - the grammar rules in a context sensitive grammar
          can be viewed as named by the focus and context
          in its left hand side
        - the grammar rules in a phrase structure grammar
          and monotonic grammar
          are generally unnamed

*** 3.1.3 Linearization of the Parse Tree

    #+begin_src rust
    grammar! {
        // grammar with named rules
        digit = Sum = Digit { A0 = A1 };
        sum   = Sum = Sum " + " Sum { A0 = A1 + A3 };
        0     = Digit = "0" { A0 = 0 };
        ...;
        9     = Digit = "9" { A0 = 9 };
    }

    // one parse tree of 3 + 5 + 1 (there are two)

    // left-to-right order for sub-note

    // postfix notation -- forth
    3 digit 5 digit sum 1 digit sum

    // prefix notation
    sum sum digit 3 digit 5 digit 1

    // prefix notation -- c style bracket
    sum (sum (digit (3),
              digit (5)),
         digit (1))

    // prefix notation -- lisp style bracket
    (sum (sum (digit 3)
              (digit 5))
         (digit 1))
    #+end_src

*** 3.2 Two Ways to Parse a Sentence

    - The basic connection between a sentence
      and the grammar it derives from
      is the parse tree,
      which describes how the grammar was used
      to produce the sentence.

      - x -
        the above feeling only apply to people
        who has a formal-grammar,
        but does not have a parser.
        On the contrary,
        for people who write parser by hand
        (instead of generating the parser by grammar,
        or even without a formal-grammar as reference.)
        they write program to generate
        structured data (parse tree) from linear string
        and the grammar is in turn the connection between
        a structured data and a linear string.

        - For example,
          from messenger RNA -- linear string
          to structured protein,
          what is the grammar?

    - There are only two techniques to do parsing,
      all the rest is technical detail and embellishment.
      (just topping over pizza)

      1. top-down parsing
         imitating the production process
         identifies the production rules in prefix order.

      2. bottom-up
         reverse the production process
         identifies the production rules in postfix order.

    - bottom-up parsing
      turns the parsing-process
      into a production-process
      by reverse the grammar

    #+begin_src rust
    grammar! {
        S = "abc" | "a" S Q;
        "b" Q "c" = "bbcc";
        "c" Q = Q "c";
    }

    // reversed grammar for parsing "aabbcc"
    grammar! {
        S <- "abc" | "a" S Q;
        "b" Q "c" <- "bbcc";
        "c" Q <- Q "c";

        I = "aabbcc";
        S = "!";
    }

    // If, starting from I, we can produce "!"
    //   we have recognized the input string,
    // and if we have kept records of what we did,
    //   we also have obtained the parse tree.
    #+end_src

*** 3.3 Non-Deterministic Automata

    - A structure can be discerned in all parsing methods:
      there is always a substituting and record-keeping machine,
      and a guiding control mechanism.

    - The substituting machine
      is called a non-deterministic automaton or NDA.

    - Every move of the NDA
      transfers some information
      from the input string to the partial parse tree.

*** 3.4.3 Type 2 Grammars

    - Almost all practical parsing is done
      using CF and FS grammars,
      and almost all problems in context-free parsing
      have been solved.

    - the evolution of one non-terminal in the sentential form
      is totally *independent* of the evolution
      of any other non-terminal,
      and, conversely, during parsing
      we can combine partial parse trees
      regardless of their histories.
      Neither is true in a context sensitive grammar.

    #+begin_src rust
    grammar! {
        Sentence = Subject Verb Object;
        Subject = "the " Noun | "a " Noun | ProperName;
        Object = "the " Noun | "a " Noun | ProperName;
        Verb = "bit" | "chased";
        Noun = "cat" | "dog";
        ProperName = ···;
    }
    #+end_src

*** 3.5 An Overview of Context-Free Parsing Methods

    - how to construct the parse-tree?
      top-down -- bottom-up

    - how to accessing the input string?
      directional -- non-directional

    - how to search in the space introduced by non-deterministic?
      depth-first -- breadth-first

    - x -
      it seems different methods are not symmetry,
      just like prefix notation and postfix notation
      are not symmetry, because of the external factor of writing.
      but what is the external factor of the parsing methods?

*** 3.5.4 Linear Methods

    - non-deterministic means non-linear

    - We can achieve linear parsing time
      by restricting the number of possible moves
      of our non-deterministic parsing automaton
      to one in each situation.
      Since the moves of such an automaton involve no choice,
      it is called a "deterministic automaton".

    - From a very global point of view
      they all use the same technique:
      - they analyse the grammar in depth
        to bring to the surface information
        that can be used to identify dead ends.
        These are then closed.

    - x -
      To design a parser generator,
      our input is user specified grammar,
      our output is parsing automaton.
      thus, we must report to the user,
      about the natural of the grammar
      - is the grammar unambiguous?
      and the feature of the automaton
      - is the automaton deterministic?
        thus we have linear parsing method?
      - maybe give hint to the user
        about how to adapt grammar to get linear parsing.

    - Note that, In parsing
      we can only preprocess grammar but not the input.

*** 3.5.5 Deterministic Top-Down and Bottom-Up Methods

    - There is only one deterministic top-down method;
      it is called LL
      - The first L stands for Left-to-right
      - the second for "identifying the Leftmost production"
        as directional top-down parsers do

    - There are quite a variety of deterministic bottom-up methods
      the most powerful being called LR
      - where again the L stands for Left-to-right
      - and the R stands for "identifying the Rightmost production"

*** 3.5.7 Generalized Linear Methods

    - When our attempt
      to construct a deterministic control mechanism fails
      and leaves us with a non-deterministic
      but almost deterministic one,
      we need not despair yet:
      we can fall back on breadth-first search
      to solve the remnants of non-determinism at run-time.

*** 3.6 The "Strength" of a Parsing Technique

    - Firstly, by "a parsing technique"
      we mean a parser generator.

    - And this partial order relation -- "strength"
      is defined by the set of grammars
      a parser generator can handle.

* 4 General Non-Directional Parsing

*** [todo] 4.1 Unger's Parsing Method

* 5 Regular Grammars and Finite-State Automata

*** 5.2 Producing from a Regular Grammar

    - When producing from a regular grammar,
      the producer needs to remember only one thing:
      which non-terminal is next.

    - it is simple to translate a regular grammar
      to a non-deterministic finite automaton

    - With regular grammars
      one is often not interested in parsing
      because the structured data is linear
      and the input string is alreay linear

    - non-deterministic finite automaton
      can be translate to deterministic finite automaton

* 6 General Directional Top-Down Parsing

*** equal number of "a" "b" -- reverse_production!

    #+begin_src rust
    grammar! {
        Start = S;
        S = "a" B | "b" A;
        A = "a" | "a" S | "b" A A;
        B = "b" | "b" S | "a" B B;
    }

    production! {
        S;
        "a" B;
        "aa" B B;
        "aab" B;
        "aabb";
    }

    reverse_production! {
        "" . "aabb" -- S;
        "" . "aabb" -- "a" B;
        "a" . "abb" -- B;
        "a" . "abb" -- "a" B B;
        "aa" . "bb" -- B B;
        "aa" . "bb" -- "b" B;
        "aab" . "b" -- B;
        "aab" . "b" -- "b";
        "aabb" . "" --;
    }
    #+end_src

*** sum -- linear_parse_tree!

    - default is left-to-right order for sub-note

    #+begin_src rust
    grammar_in_type! {
        enum Sum {
            Digit (Digit),
            Sum   (Sum, " + ", Sum),
        }
        enum Digit {
            Digit0 ("0"),
            ...,
            Digit9 ("9"),
        }
    }

    // one parse tree of 3 + 5 + 1 -- ((3 + 5) + 1)
    //   the grammar is ambiguous
    //   another parse tree is (3 + (5 + 1))

    postfix_linear_parse_tree! {
        Digit::Digit3 Sum::Digit
        Digit::Digit5 Sum::Digit Sum::Sum
        Digit::Digit1 Sum::Digit Sum::Sum
    }

    prefix_linear_parse_tree! {
        Sum::Sum
            Sum::Sum
                Sum::Digit Digit::Digit3
                Sum::Digit Digit::Digit5
            Sum::Digit Digit::Digit1
    }

    let parse_tree =
        Sum::Sum (
            Sum::Sum (
                Sum::Digit (Digit::Digit3 ()),
                Sum::Digit (Digit::Digit5 ())),
            Sum::Digit (Digit::Digit1 ()));

    assert_member! (
        Sum::ambiguous_parse ("3 + 5 + 1") .unwrap (),
        parse_tree);
    #+end_src

*** ambiguous_parse

    #+begin_src rust
    grammar! {
        // L = "a"^m "b"^n "c"^n | "a"^p "b"^p "c"^q;
        Start = S;
        S = A B | D C;
        A = "a" | "a" A;
        B = "bc" | "b" B "c";
        D = "ab" | "a" D "b";
        C = "c" | "c" C;
    }

    grammar! {
        // L = "a"^m "b"^n "c"^n | "a"^p "b"^p "c"^q;
        Start = S;
        S = A BC | AB C;
        A = "a" | "a" A;
        BC = "bc" | "b" BC "c";
        AB = "ab" | "a" AB "b";
        C = "c" | "c" C;
    }

    assert_member! (
        S::parse ("aabc") .unwrap (),
        S::A_BC (A::More (A::One ()),
                 BC::One ())
    );
    #+end_src

*** 6.3 Breadth-First Top-Down Parsing

    - The breadth-first solution to the top-down parsing problem
      is to maintain a list of all possible predictions.

    - The method is suitable for on-line parsing
      (or say, real time parsing)
      because it processes the input from left to right.

*** [todo] 6.4 Eliminating Left Recursion

*** 6.5 Depth-First (Backtracking) Parsers

    - The breadth-first method presented in the previous section
      has the disadvantage that it uses a great deal of memory.

    - The depth-first method also has a disadvantage:
      in its general form it is not suitable for on-line parsing.

    - In the depth-first method,
      when we are faced with a number of possibilities,
      we choose one and leave the other possibilities for later.

      First, we fully examine the consequences
      of the choice we just made.
      If this choice turns out to be a failure
      (or even a success, but we want all solutions),

      we roll back our actions until the present point
      and continue with the other possibilities.

    - x -
      the "analysis-stack" is our "return-stack"
      - a enum is a jojo
      - and each choice is a jo

*** 6.6 Recursive Descent

    - x -
      this is about bloody hand-written parsers,
      I will never do this (maybe except for testing)

*** grammar interpreters

    - x -
      the sentence generator we wrote
      is a formal-grammar interpreter.
      a parser generator can also be implemented as an interpreter
      thus, just like implementing languages,
      we have two ways to implement parser generator
      1. interpreter -- just a high order function
      2. compiler -- real parser generator
         generate automaton from grammar

    - k -
      and a formal-grammar is just a logic program,
      implementing parser generator
      is just implementing prolog-like logic language.

* [todo] 7 General Directional Bottom-Up Parsing

* [todo] 8 Deterministic Top-Down Parsing

* [todo] 9 Deterministic Bottom-Up Parsing

* 16 Error Handling

*** 16.1 Detection versus Recovery versus Correction

    - error detection
      - The least informative :
        "input contains syntax error(s)"
      - more informative :
        "Look, there is a syntax error
        at position so-and-so in the input, so I give up"

    - error recovery
      be able to go on parsing when one error occurs
      - the aim is to report all the errors
      - when an error occurs, no parse tree is returned
      - x -
        this is not what we want
        because it is meaningless to report error over error
        bad examples of this are compiler error of gcc and clang

    - error correction
      methods modify the input as read by the parser
      so that it becomes syntactically correct,
      usually by deleting, inserting, or changing symbols.
      - the aim is to still return a parse tree

*** 16.2 Parsing Techniques and Error Detection

    - correct-prefix property

*** 16.2.3 Error Detection in General Directional Top-Down Parsers

    - the only thing that we must remember is
      the furthest point in the input that the parser has reached,
      a kind of high-water mark.
      The first error is found right after this point.
      - x -
        we can report
        - the furthest point reached
        - the partial parse tree
        - why we can not go further here
        but we can not be sure this is the intention of the user

* 17 Practical Parser Writing and Usage

* type-constructor and kleene star
