#+title: probabilistic systems analysis

* [todo]
  - sigma-algebra
* links

  https://www.youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8

* intro

  - probability as a mathematical framework
    for reasoning about uncertainty

* probability space

  - sample space -- set of outcomes
    we record which outcome is more likely to occur compare to others
    by assigning probability to outcome.

    but to deal with continuous sample space
    we must adjust our method by assigning probability to events
    where an event is a subset of sample space

    this definition also handles discrete sample space well
    by viewing assigning probability to sets of one element

  - outcome : sample space
    event = set of outcomes = subset of sample space

  - probability axioms

    - nonnegative -
      (A : sample-space) -> P (A) >= 0

    - normalization -
      P (sample-space) = 1

    - notations about set :
      | ^  | set intersection |
      | +  | set union        |
      | =< | sub set          |
      | >= | super set        |

    - additivity -
      (A ^ B = empty-set) -> P (A + B) = P (A) + P (B)

    - countable additivity -
      the additivity axiom need to be generalized to sequence of subsets
      instead of just two subsets
      (where sequence implies countable)

    the axioms looks like integration of some function over sample-space

  - extra notes about subsets of sample-space
    ignore ugly and weird subsets
    because for some weird subsets we can not both
    assigning probability to them and maintain probability axioms
    - x -
      this also revealed set theory's inconvenience,
      but why we are not using type theory yet ?
      will type theory be even more inconvenient ?

  - x -
    dealing with subsets remembers me of relational programming

  - discrete uniform distribution
    all outcomes be equally likely
    compute probability = counting

  - continuous uniform distribution
    probability = area

* probability-space-t

  #+begin_src cicada
  probability-space-t = conj {
    sample-t : type
    event-t = set-t (sample-t)
    P : (event-t) -> unit-interval-t
    prob = P
    nonnegative : (A : event-t) -> P (A) >= 0
    normalization : P (sample-t) == 1
    additivity : (
      A : event-t
      B : event-t
      A ^ B == empty-set
    ) -> P (A + B) == P (A) + P (B)
    countable-additivity : (
      f : (nat-t -> event-t)
      [todo]
    ) -> [todo]
  }
  #+end_src

* zero probability

  - if we define probability by volume in n dimension unit space
    zero probability does not mean impossible
    zero probability only means n dimensional volume is zero

* conditional probability

  - the story goes like this,
    you know something about this uncertain world,
    and based on what you know, you build a probability model,
    by write down probabilities for different outcomes.
    then something happened, you get some new informations,
    you know more about the world,
    these new informations should change your beliefs
    about what may happen and what may not happen.

  - partial informations about random experiments and revise beliefs

  - P (A | B) := the probability of A, given that B occurred
    P (A | B) := P (A ^ B) / P (B)
    - given P (B) != 0
    - if P (B) = 0, P (A | B) is undefined

  - P (A ^ B) = P (B) * P (A | B)
    P (A ^ B) = P (A) * P (B | A)

  - with the above definition
    the proportion of probabilities in B is maintained
    suppose A1 =< B and A2 =< B
    P (A1 | B) / P (A2 | B) =
    P (A1 ^ B) / P (A2 ^ B) =
    P (A1) / P (A2)

  - we specify the probability model by a conditional probability tree
    instead of calculate conditional probability of a given model

  - P (A ^ B) = P (A) * P (B | A)

    P (A ^ B ^ C) =
    P ((A ^ B) ^ C) =
    P (A ^ B) * P ((A ^ B) | C) =
    P (A) * P (B | A) * P ((A ^ B) | C)

  - P (B) =
    P (B ^ (A + ~A)) =
    P (B ^ A) + P (B ^ ~A) =
    P (A) * P (B | A) + P (~A) * P (B | ~A)

    P (B) =
    P (B ^ (A1 + A2 + A3)) =
    P (B ^ A1) + P (B ^ A2) + P (B ^ A3) =
    P (A1) * P (B | A1) +
    P (A2) * P (B | A2) +
    P (A3) * P (B | A3)

  - P (A | B) = P (A ^ B) / P (B)
    can be interpreted as an inference problem
    suppose B, what is the probability of A

    - where P (A ^ B) and P (B) can be calculated
      by the above two sections
      which goes from P (B | Ai) to P (Ai | B)

    - bayes-rule :
      we know
      Ai => B -- P (B | Ai)
      we observe B, and we infer
      B => Ai -- P (Ai | B)

    - the => above can be viewed as
      causal relation in the sense of hume

* independence

  - P (B | A) = P (B)

  - P (A ^ B) =
    P (A) * P (B | A) =
    P (A) * P (B)

  - do not confuse independence with disjointness
    disjoint means
    P (A + B) = P (A) + P (B)

  - A and B are independent means
    the fact that A happens conveys no information about B

  - since conditional probability is probability
    independence can be generalized to conditional independence

    - x -
      conditional under B :
      (probability-space-t) -> probability-space-t
      conditional :
      (event-t, probability-space-t) -> probability-space-t

      - where a probabilistic-model is like one of our belief

  - for many events, independence is a very strong conditional
    we can also define pairwise independence
    - a weaker version of independence

* random-variable-t

  - random variable can be viewed statistically first
    - without introducing probabilistic-model

  - a random variable is a function from sample to number

  #+begin_src cicada
  random-variable-t = conj {
    state-t : type
    space : probability-space-t
    var : (space.sample-t) -> state-t
  }
  #+end_src

  - random variable is used to specify subset of sample space
    on which we can calculate probability

    - x -
      the concept of function give us a handle
      to discuss more about probabilistic-model
      to express properties of what we are interested in

      - categorical pushout ?

  - probability mass function

  #+begin_src cicada
  random-variable-t
  .mass : (this.state-t) -> unit-interval-t
  .mass = (num) => this.prob (this.var.reverse (num))
  #+end_src

  - x -
    the above formalization is rough
    we need to formalize set theory first

  - expected value -- center of mass

  #+begin_src cicada
  random-variable-t
  .mean : () -> unit-interval-t
  .mean = () => this.state-t.sum-up ((x) => {
    x * this.var.mass (x)
  })
  #+end_src

  - expected value of function composition

  - variance

* random-field-t

  #+begin_src cicada
  random-field-t = conj {
    index-t : type
    state-t : type
    idx : (index-t) -> random-variable-t (state-t)
  }

  random-field-t <: random-variable-t {
    state-t = (index-t) -> super.state-t
    space = [todo]
    var = [todo]
  }
  #+end_src
