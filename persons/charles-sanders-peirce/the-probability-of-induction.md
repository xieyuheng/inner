---
title: The probability of induction
subtitle: The fourth paper of "Illustrations of the logic of science"
author: Charles Sanders Peirce
year: 1878
---

# I

We have found that every argument derives its force from the general
truth of the class of inferences to which it belongs; and that
probability is the proportion of arguments carrying truth with them
among those of any _genus_. This is most conveniently expressed in the
nomenclature of the medieval logicians. They called the fact expressed
by a premise an _antecedent_, and that which follows from it its
_consequent_; while the leading principle, that every (or almost every)
such antecedent is followed by such a consequent, they termed the
_consequence_. Using this language, we may say that probability belongs
exclusively to _consequences_, and the probability of any consequence is
the number of times in which antecedent and consequent both occur
divided by the number of all the times in which the antecedent occurs.
From this definition are deduced the following rules for the addition
and multiplication of probabilities:

_Rule for the Addition of Probabilities._—Given the separate
probabilities of two consequences having the same antecedent and
incompatible consequents. Then the sum of these two numbers is the
probability of the consequence, that from the same antecedent one or
other of those consequents follows.

_Rule for the Multiplication of Probabilities._—Given the separate
probabilities of the two consequences, “If A then B,” and “If both A and
B, then C.” Then the product of these two numbers is the probability of
the consequence, “If A, then both B and C.”

_Special Rule for the Multiplication of Independent
Probabilities._—Given the separate probabilities of two consequences
having the same antecedents, “If A, then B,” and “If A, then C.” Suppose
that these consequences are such that the probability of the second is
equal to the probability of the consequence, “If both A and B, then C.”
Then the product of the two given numbers is equal to the probability of
the consequence, “If A, then both B and C.”

To show the working of these rules we may examine the probabilities in
regard to throwing dice. What is the probability of throwing a six with
one die? The antecedent here is the event of throwing a die; the
consequent, its turning up a six. As the die has six sides, all of which
are turned up with equal frequency, the probability of turning up any
one is 1/6. Suppose two dice are thrown, what is the probability of
throwing sixes? The probability of either coming up six is obviously the
same when both are thrown as when one is thrown—namely, 1/6. The
probability that either will come up six when the other does is also the
same as that of its coming up six whether the other does or not. The
probabilities are, therefore, independent; and, by our rule, the
probability that both events will happen together is the product of
their several probabilities, or 1/6 x 1/6. What is the probability of
throwing deuce-ace? The probability that the first die will turn up ace
and the second deuce is the same as the probability that both will turn
up sixes—namely, 1/36; the probability that the _second_ will turn up
ace and the _first_ deuce is likewise 1/36; these two events—first, ace;
second, deuce; and, second, ace; first, deuce—are incompatible. Hence
the rule for addition holds, and the probability that either will come
up ace and the other deuce is 1/36 + 1/36, or 1/18.

In this way all problems about dice, etc., may be solved. When the
number of dice thrown is supposed very large, mathematics (which may be
defined as the art of making groups to facilitate numeration) comes to
our aid with certain devices to reduce the difficulties.

# II

The conception of probability as a matter of _fact_, i.e., as the
proportion of times in which an occurrence of one kind is accompanied by
an occurrence of another kind, is termed by Mr. Venn the materialistic
view of the subject. But probability has often been regarded as being
simply the degree of belief which ought to attach to a proposition, and
this mode of explaining the idea is termed by Venn the conceptualistic
view. Most writers have mixed the two conceptions together. They, first,
define the probability of an event as the reason we have to believe that
it has taken place, which is conceptualistic; but shortly after they
state that it is the ratio of the number of cases favorable to the event
to the total number of cases favorable or contrary, and all equally
possible. Except that this introduces the thoroughly unclear idea of
cases equally possible in place of cases equally frequent, this is a
tolerable statement of the materialistic view. The pure conceptualistic
theory has been best expounded by Mr. De Morgan in his _Formal Logic_:
or, the _Calculus of Inference, Necessary and Probable._

The great difference between the two analyses is, that the
conceptualists refer probability to an event, while the materialists
make it the ratio of frequency of events of a _species_ to those of a
_genus_ over that _species_, thus _giving it two terms instead of one_.
The opposition may be made to appear as follows:

Suppose that we have two rules of inference, such that, of all the
questions to the solution of which both can be applied, the first yields
correct answers to 81/100, and incorrect answers to the remaining
19/100; while the second yields correct answers to 93/100, and incorrect
answers to the remaining 7/100. Suppose, further, that the two rules are
entirely independent as to their truth, so that the second answers
correctly 93/100 of the questions which the first answers correctly, and
also 93/100 of the questions which the first answers incorrectly, and
answers incorrectly the remaining 7/100 of the questions which the first
answers correctly, and also the remaining 7/100 of the questions which
the first answers incorrectly. Then, of all the questions to the
solution of which both rules can be applied—

    both answer correctly 93/100 of 81/100 or 93/100 x 81/100;

    the second answers correctly and the first incorrectly 93/100 of
    19/100 or 93/100 x 19/100;

    the second answers incorrectly and the first correctly 7/100 of
    81/100 or 7/100 x 81/100;

    and both answer incorrectly 7/100 of 19/100 or 7/100 x 19/100;

Suppose, now, that, in reference to any question, both give the same
answer. Then (the questions being always such as are to be answered by
_yes_ or _no_), those in reference to which their answers agree are the
same as those which both answer correctly together with those which both
answer falsely, or 93/100 x 81/100 + 7/100 x 19/100 of all. The
proportion of those which both answer correctly out of those their
answers to which agree is, therefore—

    ((93 × 81)/(100 × 100))/((93 × 81)/(100 × 100)) + ((7 × 19)/(100 ×
    100)) or (93 × 81)/((93 × 81) + (7 × 19)).

This is, therefore, the probability that, if both modes of inference
yield the same result, that result is correct. We may here conveniently
make use of another mode of expression. _Probability_ is the ratio of
the favorable cases to all the cases. Instead of expressing our result
in terms of this ratio, we may make use of another—the ratio of
favorable to unfavorable cases. This last ratio may be called the
_chance_ of an event. Then the chance of a true answer by the first mode
of inference is 81/19 and by the second is 93/7; and the chance of a
correct answer from both, when they agree, is—

    (81 × 93)/(19 × 7) or 81/19 × 93/7,

or the product of the chances of each singly yielding a true answer.

It will be seen that a chance is a quantity which may have any
magnitude, however great. An event in whose favor there is an even
chance, or 1/1, has a probability of 1/2. An argument having an even
chance can do nothing toward re-enforcing others, since according to the
rule its combination with another would only multiply the chance of the
latter by 1.

Probability and chance undoubtedly belong primarily to consequences, and
are relative to premises; but we may, nevertheless, speak of the chance
of an event absolutely, meaning by that the chance of the combination of
all arguments in reference to it which exist for us in the given state
of our knowledge. Taken in this sense it is incontestable that the
chance of an event has an intimate connection with the degree of our
belief in it. Belief is certainly something more than a mere feeling;
yet there is a feeling of believing, and this feeling does and ought to
vary with the chance of the thing believed, as deduced from all the
arguments. Any quantity which varies with the chance might, therefore,
it would seem, serve as a thermometer for the proper intensity of
belief. Among all such quantities there is one which is peculiarly
appropriate. When there is a very great chance, the feeling of belief
ought to be very intense. Absolute certainty, or an infinite chance, can
never be attained by mortals, and this may be represented appropriately
by an infinite belief. As the chance diminishes the feeling of believing
should diminish, until an even chance is reached, where it should
completely vanish and not incline either toward or away from the
proposition. When the chance becomes less, then a contrary belief should
spring up and should increase in intensity as the chance diminishes, and
as the chance almost vanishes (which it can never quite do) the contrary
belief should tend toward an infinite intensity. Now, there is one
quantity which, more simply than any other, fulfills these conditions;
it is the _logarithm_ of the chance. But there is another consideration
which must, if admitted, fix us to this choice for our thermometer. It
is that our belief ought to be proportional to the weight of evidence,
in this sense, that two arguments which are entirely independent,
neither weakening nor strengthening each other, ought, when they concur,
to produce a belief equal to the sum of the intensities of belief which
either would produce separately. Now, we have seen that the chances of
independent concurrent arguments are to be multiplied together to get
the chance of their combination, and, therefore, the quantities which
best express the intensities of belief should be such that they are to
be _added_ when the _chances_ are multiplied in order to produce the
quantity which corresponds to the combined chance. Now, the logarithm is
the only quantity which fulfills this condition. There is a general law
of sensibility, called Fechner’s psychophysical law. It is that the
intensity of any sensation is proportional to the logarithm of the
external force which produces it. It is entirely in harmony with this
law that the feeling of belief should be as the logarithm of the chance,
this latter being the expression of the state of facts which produces
the belief.

The rule for the combination of independent concurrent arguments takes a
very simple form when expressed in terms of the intensity of belief,
measured in the proposed way. It is this: Take the sum of all the
feelings of belief which would be produced separately by all the
arguments _pro_, subtract from that the similar sum for arguments _con_,
and the remainder is the feeling of belief which we ought to have on the
whole. This is a proceeding which men often resort to, under the name of
_balancing reasons_.

These considerations constitute an argument in favor of the
conceptualistic view. The kernel of it is that the conjoint probability
of all the arguments in our possession, with reference to any fact, must
be intimately connected with the just degree of our belief in that fact;
and this point is supplemented by various others showing the consistency
of the theory with itself and with the rest of our knowledge.

But probability, to have any value at all, must express a fact. It is,
therefore, a thing to be inferred upon evidence. Let us, then, consider
for a moment the formation of a belief of probability. Suppose we have a
large bag of beans from which one has been secretly taken at random and
hidden under a thimble. We are now to form a probable judgment of the
color of that bean, by drawing others singly from the bag and looking at
them, each one to be thrown back, and the whole well mixed up after each
drawing. Suppose the first drawing is white and the next black. We
conclude that there is not an immense preponderance of either color, and
that there is something like an even chance that the bean under the
thimble is black. But this judgment may be altered by the next few
drawings. When we have drawn ten times, if 4, 5, or 6, are white, we
have more confidence that the chance is even. When we have drawn a
thousand times, if about half have been white, we have great confidence
in this result. We now feel pretty sure that, if we were to make a large
number of bets upon the color of single beans drawn from the bag, we
could approximately insure ourselves in the long run by betting each
time upon the white, a confidence which would be entirely wanting if,
instead of sampling the bag by 1,000 drawings, we had done so by only
two. Now, as the whole utility of probability is to insure us in the
long run, and as that assurance depends, not merely on the value of the
chance, but also on the accuracy of the evaluation, it follows that we
ought not to have the same feeling of belief in reference to all events
of which the chance is even. In short, to express the proper state of
our belief, not _one_ number but _two_ are requisite, the first
depending on the inferred probability, the second on the amount of
knowledge on which that probability is based.[41] It is true that when
our knowledge is very precise, when we have made many drawings from the
bag, or, as in most of the examples in the books, when the total
contents of the bag are absolutely known, the number which expresses the
uncertainty of the assumed probability and its liability to be changed
by further experience may become insignificant, or utterly vanish. But,
when our knowledge is very slight, this number may be even more
important than the probability itself; and when we have no knowledge at
all this completely overwhelms the other, so that there is no sense in
saying that the chance of the totally unknown event is even (for what
expresses absolutely no fact has absolutely no meaning), and what ought
to be said is that the chance is entirely indefinite. We thus perceive
that the conceptualistic view, though answering well enough in some
cases, is quite inadequate.

Suppose that the first bean which we drew from our bag were black. That
would constitute an argument, no matter how slender, that the bean under
the thimble was also black. If the second bean were also to turn out
black, that would be a second independent argument reënforcing the
first. If the whole of the first twenty beans drawn should prove black,
our confidence that the hidden bean was black would justly attain
considerable strength. But suppose the twenty-first bean were to be
white and that we were to go on drawing until we found that we had drawn
1,010 black beans and 990 white ones. We should conclude that our first
twenty beans being black was simply an extraordinary accident, and that
in fact the proportion of white beans to black was sensibly equal, and
that it was an even chance that the hidden bean was black. Yet according
to the rule of _balancing reasons_, since all the drawings of black
beans are so many independent arguments in favor of the one under the
thimble being black, and all the white drawings so many against it, an
excess of twenty black beans ought to produce the same degree of belief
that the hidden bean was black, whatever the total number drawn.

In the conceptualistic view of probability, complete ignorance, where
the judgment ought not to swerve either toward or away from the
hypothesis, is represented by the probability 1/2.[42]

But let us suppose that we are totally ignorant what colored hair the
inhabitants of Saturn have. Let us, then, take a color-chart in which
all possible colors are shown shading into one another by imperceptible
degrees. In such a chart the relative areas occupied by different
classes of colors are perfectly arbitrary. Let us inclose such an area
with a closed line, and ask what is the chance on conceptualistic
principles that the color of the hair of the inhabitants of Saturn falls
within that area? The answer cannot be indeterminate because we must be
in some state of belief; and, indeed, conceptualistic writers do not
admit indeterminate probabilities. As there is no certainty in the
matter, the answer lies between _zero_ and _unity_. As no numerical
value is afforded by the data, the number must be determined by the
nature of the scale of probability itself, and not by calculation from
the data. The answer can, therefore, only be one-half, since the
judgment should neither favor nor oppose the hypothesis. What is true of
this area is true of any other one; and it will equally be true of a
third area which embraces the other two. But the probability for each of
the smaller areas being one-half, that for the larger should be at least
unity, which is absurd.

# III

All our reasonings are of two kinds: 1. _Explicative_, _analytic_, or
_deductive_; 2. _Amplifiative_, _synthetic_, or (loosely speaking)
_inductive_. In explicative reasoning, certain facts are first laid down
in the premises. These facts are, in every case, an inexhaustible
multitude, but they may often be summed up in one simple proposition by
means of some regularity which runs through them all. Thus, take the
proposition that Socrates was a man; this implies (to go no further)
that during every fraction of a second of his whole life (or, if you
please, during the greater part of them) he was a man. He did not at one
instant appear as a tree and at another as a dog; he did not flow into
water, or appear in two places at once; you could not put your finger
through him as if he were an optical image, etc. Now, the facts being
thus laid down, some order among some of them, not particularly made use
of for the purpose of stating them, may perhaps be discovered; and this
will enable us to throw part or all of them into a new statement, the
possibility of which might have escaped attention. Such a statement will
be the conclusion of an analytic inference. Of this sort are all
mathematical demonstrations. But synthetic reasoning is of another kind.
In this case the facts summed up in the conclusion are not among those
stated in the premises. They are different facts, as when one sees that
the tide rises _m_ times and concludes that it will rise the next time.
These are the only inferences which increase our real knowledge, however
useful the others may be.

In any problem in probabilities, we have given the relative frequency of
certain events, and we perceive that in these facts the relative
frequency of another event is given in a hidden way. This being stated
makes the solution. This is, therefore, mere explicative reasoning, and
is evidently entirely inadequate to the representation of synthetic
reasoning, which goes out beyond the facts given in the premises. There
is, therefore, a manifest impossibility in so tracing out any
probability for a synthetic conclusion.

Most treatises on probability contain a very different doctrine. They
state, for example, that if one of the ancient denizens of the shores of
the Mediterranean, who had never heard of tides, had gone to the bay of
Biscay, and had there seen the tide rise, say _m_ times, he could know
that there was a probability equal to

    (m + 1)/(m + 2)

that it would rise the next time. In a well-known work by Quetelet, much
stress is laid on this, and it is made the foundation of a theory of
inductive reasoning.

But this solution betrays its origin if we apply it to the case in which
the man has never seen the tide rise at all; that is, if we put _m_ = 0.
In this case, the probability that it will rise the next time comes out
1/2, or, in other words, the solution involves the conceptualistic
principle that there is an even chance of a totally unknown event. The
manner in which it has been reached has been by considering a number of
urns all containing the same number of balls, part white and part black.
One urn contains all white balls, another one black and the rest white,
a third two black and the rest white, and so on, one urn for each
proportion, until an urn is reached containing only black balls. But the
only possible reason for drawing any analogy between such an arrangement
and that of Nature is the principle that alternatives of which we know
nothing must be considered as equally probable. But this principle is
absurd. There is an indefinite variety of ways of enumerating the
different possibilities, which, on the application of this principle,
would give different results. If there be any way of enumerating the
possibilities so as to make them all equal, it is not that from which
this solution is derived, but is the following: Suppose we had an
immense granary filled with black and white balls well mixed up; and
suppose each urn were filled by taking a fixed number of balls from this
granary quite at random. The relative number of white balls in the
granary might be anything, say one in three. Then in one-third of the
urns the first ball would be white, and in two-thirds black. In
one-third of those urns of which the first ball was white, and also in
one-third of those in which the first ball was black, the second ball
would be white. In this way, we should have a distribution like that
shown in the following table, where _w_ stands for a white ball and _b_
for a black one. The reader can, if he chooses, verify the table for
himself.

    wwww.

    wwwb.   wwbw.   wbww.   bwww.
    wwwb.   wwbw.   wbww.   bwww.

    wwbb.   wbwb.   bwwb.   wbbw.   bwbw.   bbww.
    wwbb.   wbwb.   bwwb.   wbbw.   bwbw.   bbww.
    wwbb.   wbwb.   bwwb.   wbbw.   bwbw.   bbww.
    wwbb.   wbwb.   bwwb.   wbbw.   bwbw.   bbww.

    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.
    wbbb.   bwbb.   bbwb.   bbbw.

    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.
    bbbb.

In the second group, where there is one b, there are two sets just
alike; in the third there are 4, in the fourth 8, and in the fifth 16,
doubling every time. This is because we have supposed twice as many
black balls in the granary as white ones; had we supposed 10 times as
many, instead of

           1, 2, 4, 8, 16

sets we should have had

           1, 10, 100, 1000, 10000

sets; on the other hand, had the numbers of black and white balls in the
granary been even, there would have been but one set in each group. Now
suppose two balls were drawn from one of these urns and were found to be
both white, what would be the probability of the next one being white?
If the two drawn out were the first two put into the urns, and the next
to be drawn out were the third put in, then the probability of this
third being white would be the same whatever the colors of the first
two, for it has been supposed that just the same proportion of urns has
the third ball white among those which have the first two _white-white_,
_white-black_, _black-white_, and _black-black_. Thus, in this case, the
chance of the third ball being white would be the same whatever the
first two were. But, by inspecting the table, the reader can see that in
each group all orders of the balls occur with equal frequency, so that
it makes no difference whether they are drawn out in the order they were
put in or not. Hence the colors of the balls already drawn have no
influence on the probability of any other being white or black.

Now, if there be any way of enumerating the possibilities of Nature so
as to make them equally probable, it is clearly one which should make
one arrangement or combination of the elements of Nature as probable as
another, that is, a distribution like that we have supposed, and it,
therefore, appears that the assumption that any such thing can be done,
leads simply to the conclusion that reasoning from past to future
experience is absolutely worthless. In fact, the moment that you assume
that the chances in favor of that of which we are totally ignorant are
even, the problem about the tides does not differ, in any arithmetical
particular, from the case in which a penny (known to be equally likely
to come up heads and tails) should turn up heads _m_ times successively.
In short, it would be to assume that Nature is a pure chaos, or chance
combination of independent elements, in which reasoning from one fact to
another would be impossible; and since, as we shall hereafter see, there
is no judgment of pure observation without reasoning, it would be to
suppose all human cognition illusory and no real knowledge possible. It
would be to suppose that if we have found the order of Nature more or
less regular in the past, this has been by a pure run of luck which we
may expect is now at an end. Now, it may be we have no scintilla of
proof to the contrary, but reason is unnecessary in reference to that
belief which is of all the most settled, which nobody doubts or can
doubt, and which he who should deny would stultify himself in so doing.

The relative probability of this or that arrangement of Nature is
something which we should have a right to talk about if universes were
as plenty as blackberries, if we could put a quantity of them in a bag,
shake them well up, draw out a sample, and examine them to see what
proportion of them had one arrangement and what proportion another. But,
even in that case, a higher universe would contain us, in regard to
whose arrangements the conception of probability could have no
applicability.

# IV

We have examined the problem proposed by the conceptualists, which,
translated into clear language, is this: Given a synthetic conclusion;
required to know out of all possible states of things how many will
accord, to any assigned extent, with this conclusion; and we have found
that it is only an absurd attempt to reduce synthetic to analytic
reason, and that no definite solution is possible.

But there is another problem in connection with this subject. It is
this: Given a certain state of things, required to know what proportion
of all synthetic inferences relating to it will be true within a given
degree of approximation. Now, there is no difficulty about this problem
(except for its mathematical complication); it has been much studied,
and the answer is perfectly well known. And is not this, after all, what
we want to know much rather than the other? Why should we want to know
the probability that the fact will accord with our conclusion? That
implies that we are interested in all possible worlds, and not merely
the one in which we find ourselves placed. Why is it not much more to
the purpose to know the probability that our conclusion will accord with
the fact? One of these questions is the first above stated and the other
the second, and I ask the reader whether, if people, instead of using
the word probability without any clear apprehension of their own
meaning, had always spoken of relative frequency, they could have failed
to see that what they wanted was not to follow along the synthetic
procedure with an analytic one, in order to find the probability of the
conclusion; but, on the contrary, to begin with the fact at which the
synthetic inference aims, and follow back to the facts it uses for
premises in order to see the probability of their being such as will
yield the truth.

As we cannot have an urn with an infinite number of balls to represent
the inexhaustibleness of Nature, let us suppose one with a finite
number, each ball being thrown back into the urn after being drawn out,
so that there is no exhaustion of them. Suppose one ball out of three is
white and the rest black, and that four balls are drawn. Then the table
on pages 95-96 represents the relative frequency of the different ways
in which these balls might be drawn. It will be seen that if we should
judge by these four balls of the proportion in the urn, 32 times out of
81 we should find it 1/4, and 24 times out of 81 we should find it 1/2,
the truth being 1/3. To extend this table to high numbers would be great
labor, but the mathematicians have found some ingenious ways of
reckoning what the numbers would be. It is found that, if the true
proportion of white balls is _p_, and _s_ balls are drawn, then the
error of the proportion obtained by the induction will be—

    half the time within 0.477 √((2p(1-p))/s)
    9 times out of 10 within 1.163 √((2p(1-p))/s)
    99 times out of 100 within 1.821 √((2p(1-p))/s)
    999 times out of 1,000 within 2.328 √((2p(1-p))/s)
    9,999 times out of 10,000 within 2.751 √((2p(1-p))/s)
    9,999,999,999 times out of 10,000,000,000 within 4.77 √((2p(1-p))/s)

The use of this may be illustrated by an example. By the census of 1870,
it appears that the proportion of males among native white children
under one year old was 0.5082, while among colored children of the same
age the proportion was only 0.4977. The difference between these is
0.0105, or about one in a 100. Can this be attributed to chance, or
would the difference always exist among a great number of white and
colored children under like circumstances? Here _p_ may be taken at 1/2;
hence `2 p (1-p)` is also 1/2. The number of white children counted was
near 1,000,000; hence the fraction whose square-root is to be taken is
about 1/2000000. The root is about 1/1400, and this multiplied by 0.477
gives about 0.0003 as the probable error in the ratio of males among the
whites as obtained from the induction. The number of black children was
about 150,000, which gives 0.0008 for the probable error. We see that
the actual discrepancy is ten times the sum of these, and such a result
would happen, according to our table, only once out of 10,000,000,000
censuses, in the long run.

It may be remarked that when the real value of the probability sought
inductively is either very large or very small, the reasoning is more
secure. Thus, suppose there were in reality one white ball in 100 in a
certain urn, and we were to judge of the number by 100 drawings. The
probability of drawing no white ball would be 366/1000; that of drawing
one white ball would be 370/1000; that of drawing two would be 185/1000;
that of drawing three would be 61/1000; that of drawing four would be
15/1000; that of drawing five would be only 3/1000, etc. Thus we should
be tolerably certain of not being in error by more than one ball in 100.

It appears, then, that in one sense we can, and in another we cannot,
determine the probability of synthetic inference. When I reason in this
way:

                Ninety-nine Cretans in a hundred are liars;
                But Epimenides is a Cretan;
                Therefore, Epimenides is a liar:—

I know that reasoning similar to that would carry truth 99 times in 100.
But when I reason in the opposite direction:

         Minos, Sarpedon, Rhadamanthus, Deucalion, and Epimenides,
         are all the Cretans I can think of;
         But these were all atrocious liars,
         Therefore, pretty much all Cretans must have been liars;

I do not in the least know how often such reasoning would carry me
right. On the other hand, what I do know is that some definite
proportion of Cretans must have been liars, and that this proportion can
be probably approximated to by an induction from five or six instances.
Even in the worst case for the probability of such an inference, that in
which about half the Cretans are liars, the ratio so obtained would
probably not be in error by more than 1/6. So much I know; but, then, in
the present case the inference is that pretty much all Cretans are
liars, and whether there may not be a special improbability in that I do
not know.

# V

Late in the last century, Immanuel Kant asked the question, “How are
synthetical judgments _a priori_ possible?” By synthetical judgments he
meant such as assert positive fact and are not mere affairs of
arrangement; in short, judgments of the kind which synthetical reasoning
produces, and which analytic reasoning cannot yield. By _a priori_
judgments he meant such as that all outward objects are in space, every
event has a cause, etc., propositions which according to him can never
be inferred from experience. Not so much by his answer to this question
as by the mere asking of it, the current philosophy of that time was
shattered and destroyed, and a new epoch in its history was begun. But
before asking _that_ question he ought to have asked the more general
one, “How are any synthetical judgments at all possible?” How is it that
a man can observe one fact and straightway pronounce judgment concerning
another different fact not involved in the first? Such reasoning, as we
have seen, has, at least in the usual sense of the phrase, no definite
probability; how, then, can it add to our knowledge? This is a strange
paradox; the Abbé Gratry says it is a miracle, and that every true
induction is an immediate inspiration from on high.[43] I respect this
explanation far more than many a pedantic attempt to solve the question
by some juggle with probabilities, with the forms of syllogism, or what
not. I respect it because it shows an appreciation of the depth of the
problem, because it assigns an adequate cause, and because it is
intimately connected—as the true account should be—with a general
philosophy of the universe. At the same time, I do not accept this
explanation, because an explanation should tell _how_ a thing is done,
and to assert a perpetual miracle seems to be an abandonment of all hope
of doing that, without sufficient justification.

It will be interesting to see how the answer which Kant gave to his
question about synthetical judgments _a priori_ will appear if extended
to the question of synthetical judgments in general. That answer is,
that synthetical judgments _a priori_ are possible because whatever is
universally true is involved in the conditions of experience. Let us
apply this to a general synthetical reasoning. I take from a bag a
handful of beans; they are all purple, and I infer that all the beans in
the bag are purple. How can I do that? Why, upon the principle that
whatever is universally true of my experience (which is here the
appearance of these different beans) is involved in the condition of
experience. The condition of this special experience is that all these
beans were taken from that bag. According to Kant’s principle, then,
whatever is found true of all the beans drawn from the bag must find its
explanation in some peculiarity of the contents of the bag. This is a
satisfactory statement of the principle of induction.

When we draw a deductive or analytic conclusion, our rule of inference
is that facts of a certain general character are either invariably or in
a certain proportion of cases accompanied by facts of another general
character. Then our premise being a fact of the former class, we infer
with certainty or with the appropriate degree of probability the
existence of a fact of the second class. But the rule for synthetic
inference is of a different kind. When we sample a bag of beans we do
not in the least assume that the fact of some beans being purple
involves the necessity or even the probability of other beans being so.
On the contrary, the conceptualistic method of treating probabilities,
which really amounts simply to the deductive treatment of them, when
rightly carried out leads to the result that a synthetic inference has
just an even chance in its favor, or in other words is absolutely
worthless. The color of one bean is entirely independent of that of
another. But synthetic inference is founded upon a classification of
facts, not according to their characters, but according to the manner of
obtaining them. Its rule is, that a number of facts obtained in a given
way will in general more or less resemble other facts obtained in the
same way; or, _experiences whose conditions are the same will have the
same general characters_.

In the former case, we know that premises precisely similar in form to
those of the given ones will yield true conclusions, just once in a
calculable number of times. In the latter case, we only know that
premises obtained under circumstances similar to the given ones (though
perhaps themselves very different) will yield true conclusions, at least
once in a calculable number of times. We may express this by saying that
in the case of analytic inference we know the probability of our
conclusion (if the premises are true), but in the case of synthetic
inferences we only know the degree of trustworthiness of our proceeding.
As all knowledge comes from synthetic inference, we must equally infer
that all human certainty consists merely in our knowing that the
processes by which our knowledge has been derived are such as must
generally have led to true conclusions.

Though a synthetic inference cannot by any means be reduced to
deduction, yet that the rule of induction will hold good in the long run
may be deduced from the principle that reality is only the object of the
final opinion to which sufficient investigation would lead. That belief
gradually tends to fix itself under the influence of inquiry is, indeed,
one of the facts with which logic sets out.

# Footnotes

- Footnote 40:

  _Popular Science Monthly_, April, 1878.

- Footnote 41:

  Strictly we should need an infinite series of numbers each depending
  on the probable error of the last.

- Footnote 42:

  “Perfect indecision, belief inclining neither way, an even chance.”—DE
  MORGAN, p. 182.

- Footnote 43:

  _Logique_. The same is true, according to him, of every performance of
  a differentiation, but not of integration. He does not tell us whether
  it is the supernatural assistance which makes the former process so
  much the easier.
