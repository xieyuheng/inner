---
title: The doctrine of chances
author: Charles Sanders Peirce
year: 1878
---

# I

It is a common observation that a science first begins to be exact when
it is quantitatively treated. What are called the exact sciences are no
others than the mathematical ones. Chemists reasoned vaguely until
Lavoisier showed them how to apply the balance to the verification of
their theories, when chemistry leaped suddenly into the position of the
most perfect of the classificatory sciences. It has thus become so
precise and certain that we usually think of it along with optics,
thermotics, and electrics. But these are studies of general laws, while
chemistry considers merely the relations and classification of certain
objects; and belongs, in reality, in the same category as systematic
botany and zoölogy. Compare it with these last, however, and the
advantage that it derives from its quantitative treatment is very
evident.

The rudest numerical scales, such as that by which the mineralogists
distinguish the different degrees of hardness, are found useful. The
mere counting of pistils and stamens sufficed to bring botany out of
total chaos into some kind of form. It is not, however, so much from
_counting_ as from _measuring_, not so much from the conception of
number as from that of continuous quantity, that the advantage of
mathematical treatment comes. Number, after all, only serves to pin us
down to a precision in our thoughts which, however beneficial, can
seldom lead to lofty conceptions, and frequently descends to pettiness.
Of those two faculties of which Bacon speaks, that which marks
differences and that which notes resemblances, the employment of number
can only aid the lesser one; and the excessive use of it must tend to
narrow the powers of the mind. But the conception of continuous quantity
has a great office to fulfill, independently of any attempt at
precision. Far from tending to the exaggeration of differences, it is
the direct instrument of the finest generalizations. When a naturalist
wishes to study a species, he collects a considerable number of
specimens more or less similar. In contemplating them, he observes
certain ones which are more or less alike in some particular respect.
They all have, for instance, a certain S-shaped marking. He observes
that they are not _precisely_ alike, in this respect; the S has not
precisely the same shape, but the differences are such as to lead him to
believe that forms could be found intermediate between any two of those
he possesses. He, now, finds other forms apparently quite dissimilar—say
a marking in the form of a C—and the question is, whether he can find
intermediate ones which will connect these latter with the others. This
he often succeeds in doing in cases where it would at first be thought
impossible; whereas he sometimes finds those which differ, at first
glance, much less, to be separated in Nature by the non-occurrence of
intermediaries. In this way, he builds up from the study of Nature a new
general conception of the character in question. He obtains, for
example, an idea of a leaf which includes every part of the flower, and
an idea of a vertebra which includes the skull. I surely need not say
much to show what a logical engine there is here. It is the essence of
the method of the naturalist.[35] How he applies it first to one
character, and then to another, and finally obtains a notion of a
species of animals, the differences between whose members, however
great, are confined within limits, is a matter which does not here
concern us. The whole method of classification must be considered later;
but, at present, I only desire to point out that it is by taking
advantage of the idea of continuity, or the passage from one form to
another by insensible degrees, that the naturalist builds his
conceptions. Now, the naturalists are the great builders of conceptions;
there is no other branch of science where so much of this work is done
as in theirs; and we must, in great measure, take them for our teachers
in this important part of logic. And it will be found everywhere that
the idea of continuity is a powerful aid to the formation of true and
fruitful conceptions. By means of it, the greatest differences are
broken down and resolved into differences of degree, and the incessant
application of it is of the greatest value in broadening our
conceptions. I propose to make a great use of this idea in the present
series of papers; and the particular series of important fallacies,
which, arising from a neglect of it, have desolated philosophy, must
further on be closely studied. At present, I simply call the reader’s
attention to the utility of this conception.

In studies of numbers, the idea of continuity is so indispensable, that
it is perpetually introduced even where there is no continuity in fact,
as where we say that there are in the United States 10.7 inhabitants per
square mile, or that in New York 14.72 persons live in the average
house.[36] Another example is that law of the distribution of errors
which Quetelet, Galton, and others, have applied with so much success to
the study of biological and social matters. This application of
continuity to cases where it does not really exist illustrates, also,
another point which will hereafter demand a separate study, namely, the
great utility which fictions sometimes have in science.

# II

The theory of probabilities is simply the science of logic
quantitatively treated. There are two conceivable certainties with
reference to any hypothesis, the certainty of its truth and the
certainty of its falsity. The numbers _one_ and _zero_ are appropriated,
in this calculus, to marking these extremes of knowledge; while
fractions having values intermediate between them indicate, as we may
vaguely say, the degrees in which the evidence leans toward one or the
other. The general problem of probabilities is, from a given state of
facts, to determine the numerical probability of a possible fact. This
is the same as to inquire how much the given facts are worth, considered
as evidence to prove the possible fact. Thus the problem of
probabilities is simply the general problem of logic.

- **Xie:** The vaguely saying of probability
  "the degrees in which the evidence leans toward one or the other",
  is the same as the **Bayesian interpretation of probability**,
  according to which _probabilities encode degrees of belief about events in the world_
  and data are used to strengthen, update, or weaken those degrees of belief.

- **Xie:** The probability we want to determine
  is of the form:

  ```
  P(evidence -> possible_fact)
  ```

  or written as conditional probability:

  ```
  P(possible_fact | evidence)
  ```

Probability is a continuous quantity, so that great advantages may be
expected from this mode of studying logic. Some writers have gone so far
as to maintain that, by means of the calculus of chances, every solid
inference may be represented by legitimate arithmetical operations upon
the numbers given in the premises. If this be, indeed, true, the great
problem of logic, how it is that the observation of one fact can give us
knowledge of another independent fact, is reduced to a mere question of
arithmetic. It seems proper to examine this pretension before
undertaking any more recondite solution of the paradox.

But, unfortunately, writers on probabilities are not agreed in regard to
this result. This branch of mathematics is the only one, I believe, in
which good writers frequently get results entirely erroneous. In
elementary geometry the reasoning is frequently fallacious, but
erroneous conclusions are avoided; but it may be doubted if there is a
single extensive treatise on probabilities in existence which does not
contain solutions absolutely indefensible. This is partly owing to the
want of any regular method of procedure; for the subject involves too
many subtilties to make it easy to put its problems into equations
without such an aid. But, beyond this, the fundamental principles of its
calculus are more or less in dispute. In regard to that class of
questions to which it is chiefly applied for practical purposes, there
is comparatively little doubt; but in regard to others to which it has
been sought to extend it, opinion is somewhat unsettled.

This last class of difficulties can only be entirely overcome by making
the idea of probability perfectly clear in our minds in the way set
forth in our last paper.

# III

To get a clear idea of what we mean by probability, we have to consider
what real and sensible difference there is between one degree of
probability and another.

The character of probability belongs primarily, without doubt, to
certain inferences. Locke explains it as follows: After remarking that
the mathematician positively knows that the sum of the three angles of a
triangle is equal to two right angles because he apprehends the
geometrical proof, he thus continues: “But another man who never took
the pains to observe the demonstration, hearing a mathematician, a man
of credit, affirm the three angles of a triangle to be equal to two
right ones, _assents_ to it; i.e., receives it for true. In which case
the foundation of his assent is the probability of the thing, the proof
being such as, for the most part, carries truth with it; the man on
whose testimony he receives it not being wont to affirm anything
contrary to, or besides his knowledge, especially in matters of this
kind.” The celebrated _Essay concerning Human Understanding_ contains
many passages which, like this one, make the first steps in profound
analyses which are not further developed. It was shown in the first of
these papers that the validity of an inference does not depend on any
tendency of the mind to accept it, however strong such tendency may be;
but consists in the real fact that, when premises like those of the
argument in question are true, conclusions related to them like that of
this argument are also true. It was remarked that in a logical mind an
argument is always conceived as a member of a _genus_ of arguments all
constructed in the same way, and such that, when their premises are real
facts, their conclusions are so also. If the argument is demonstrative,
then this is always so; if it is only probable, then it is for the most
part so. As Locke says, the probable argument is “_such as_ for the most
part carries truth with it.”

According to this, that real and sensible difference between one degree
of probability and another, in which the meaning of the distinction
lies, is that in the frequent employment of two different modes of
inference, one will carry truth with it oftener than the other. It is
evident that this is the only difference there is in the existing fact.
Having certain premises, a man draws a certain conclusion, and as far as
this inference alone is concerned the only possible practical question
is whether that conclusion is true or not, and between existence and
non-existence there is no middle term. “Being only is and nothing is
altogether not,” said Parmenides; and this is in strict accordance with
the analysis of the conception of reality given in the last paper. For
we found that the distinction of reality and fiction depends on the
supposition that sufficient investigation would cause one opinion to be
universally received and all others to be rejected. That presupposition,
involved in the very conceptions of reality and figment, involves a
complete sundering of the two. It is the heaven-and-hell idea in the
domain of thought. But, in the long run, there is a real fact which
corresponds to the idea of probability, and it is that a given mode of
inference sometimes proves successful and sometimes not, and that in a
ratio ultimately fixed. As we go on drawing inference after inference of
the given kind, during the first ten or hundred cases the ratio of
successes may be expected to show considerable fluctuations; but when we
come into the thousands and millions, these fluctuations become less and
less; and if we continue long enough, the ratio will approximate toward
a fixed limit. We may, therefore, define the probability of a mode of
argument as the proportion of cases in which it carries truth with it.

- **Xie:** At the beginning probability is vaguely defined as "degrees of belief",
  but now, the "real and sensible" definition of probability
  is the frequency of data (the observation).

The inference from the premise, A, to the conclusion, B, depends, as we
have seen, on the guiding principle, that if a fact of the class A is
true, a fact of the class B is true. The probability consists of the
fraction whose numerator is the number of times in which both A and B
are true, and whose denominator is the total number of times in which A
is true, whether B is so or not. Instead of speaking of this as the
probability of the inference, there is not the slightest objection to
calling it the probability that, if A happens, B happens. But to speak
of the probability of the event B, without naming the condition, really
has no meaning at all. It is true that when it is perfectly obvious what
condition is meant, the ellipsis may be permitted. But we should avoid
contracting the habit of using language in this way (universal as the
habit is), because it gives rise to a vague way of thinking, as if the
action of causation might either determine an event to happen or
determine it not to happen, or leave it more or less free to happen or
not, so as to give rise to an _inherent_ chance in regard to its
occurrence.[37] It is quite clear to me that some of the worst and most
persistent errors in the use of the doctrine of chances have arisen from
this vicious mode of expression.[38]

- **Xie:** We should use conditional probability as the primitive,
  instead of viewing probability of event as primitive,
  because conditional probability is the probability of inference.

# IV

But there remains an important point to be cleared up. According to what
has been said, the idea of probability essentially belongs to a kind of
inference which is repeated indefinitely. An individual inference must
be either true or false, and can show no effect of probability; and,
therefore, in reference to a single case considered in itself,
probability can have no meaning. Yet if a man had to choose between
drawing a card from a pack containing twenty-five red cards and a black
one, or from a pack containing twenty-five black cards and a red one,
and if the drawing of a red card were destined to transport him to
eternal felicity, and that of a black one to consign him to everlasting
woe, it would be folly to deny that he ought to prefer the pack
containing the larger portion of red cards, although, from the nature of
the risk, it could not be repeated. It is not easy to reconcile this
with our analysis of the conception of chance. But suppose he should
choose the red pack, and should draw the wrong card, what consolation
would he have? He might say that he had acted in accordance with reason,
but that would only show that his reason was absolutely worthless. And
if he should choose the right card, how could he regard it as anything
but a happy accident? He could not say that if he had drawn from the
other pack, he might have drawn the wrong one, because an hypothetical
proposition such as, “if A, then B,” means nothing with reference to a
single case. Truth consists in the existence of a real fact
corresponding to the true proposition. Corresponding to the proposition,
“if A, then B,” there may be the fact that _whenever_ such an event as A
happens such an event as B happens. But in the case supposed, which has
no parallel as far as this man is concerned, there would be no real fact
whose existence could give any truth to the statement that, if he had
drawn from the other pack, he might have drawn a black card. Indeed,
since the validity of an inference consists in the truth of the
hypothetical proposition that _if_ the premises be true the conclusion
will also be true, and since the only real fact which can correspond to
such a proposition is that whenever the antecedent is true the
consequent is so also, it follows that there can be no sense in
reasoning in an isolated case, at all.

These considerations appear, at first sight, to dispose of the
difficulty mentioned. Yet the case of the other side is not yet
exhausted. Although probability will probably manifest its effect in,
say, a thousand risks, by a certain proportion between the numbers of
successes and failures, yet this, as we have seen, is only to say that
it certainly will, at length, do so. Now the number of risks, the number
of probable inferences, which a man draws in his whole life, is a finite
one, and he cannot be absolutely _certain_ that the mean result will
accord with the probabilities at all. Taking all his risks collectively,
then, it cannot be certain that they will not fail, and his case does
not differ, except in degree, from the one last supposed. It is an
indubitable result of the theory of probabilities that every gambler, if
he continues long enough, must ultimately be ruined. Suppose he tries
the martingale, which some believe infallible, and which is, as I am
informed, disallowed in the gambling-houses. In this method of playing,
he first bets say $1; if he loses it he bets $2; if he loses that he
bets $4; if he loses that he bets $8; if he then gains he has lost
`1 + 2 + 4 = 7`, and he has gained $1 more; and no matter how many bets he
loses, the first one he gains will make him $1 richer than he was in the
beginning. In that way, he will probably gain at first; but, at last,
the time will come when the run of luck is so against him that he will
not have money enough to double, and must, therefore, let his bet go.
This will _probably_ happen before he has won as much as he had in the
first place, so that this run against him will leave him poorer than he
began; some time or other it will be sure to happen. It is true that
there is always a possibility of his winning any sum the bank can pay,
and we thus come upon a celebrated paradox that, though he is certain to
be ruined, the value of his expectation calculated according to the
usual rules (which omit this consideration) is large. But, whether a
gambler plays in this way or any other, the same thing is true, namely,
that if he plays long enough he will be sure some time to have such a
run against him as to exhaust his entire fortune. The same thing is true
of an insurance company. Let the directors take the utmost pains to be
independent of great conflagrations and pestilences, their actuaries can
tell them that, according to the doctrine of chances, the time must
come, at last, when their losses will bring them to a stop. They may
tide over such a crisis by extraordinary means, but then they will start
again in a weakened state, and the same thing will happen again all the
sooner. An actuary might be inclined to deny this, because he knows that
the expectation of his company is large, or perhaps (neglecting the
interest upon money) is infinite. But calculations of expectations leave
out of account the circumstance now under consideration, which reverses
the whole thing. However, I must not be understood as saying that
insurance is on this account unsound, more than other kinds of business.
All human affairs rest upon probabilities, and the same thing is true
everywhere. If man were immortal he could be perfectly sure of seeing
the day when everything in which he had trusted should betray his trust,
and, in short, of coming eventually to hopeless misery. He would break
down, at last, as every good fortune, as every dynasty, as every
civilization does. In place of this we have death.

But what, without death, would happen to every man, with death must
happen to some man. At the same time, death makes the number of our
risks, of our inferences, finite, and so makes their mean result
uncertain. The very idea of probability and of reasoning rests on the
assumption that this number is indefinitely great. We are thus landed in
the same difficulty as before, and I can see but one solution of it. It
seems to me that we are driven to this, that logicality inexorably
requires that our interests shall _not_ be limited. They must not stop
at our own fate, but must embrace the whole community. This community,
again, must not be limited, but must extend to all races of beings with
whom we can come into immediate or mediate intellectual relation. It
must reach, however vaguely, beyond this geological epoch, beyond all
bounds. He who would not sacrifice his own soul to save the whole world,
is, as it seems to me, illogical in all his inferences, collectively.
Logic is rooted in the social principle.

- **Xie:** Maybe "social principle" is not needed,
  and it is sufficient for man to be able to imagine
  many occurrences of his inference.

To be logical men should not be selfish; and, in point of fact, they are
not so selfish as they are thought. The willful prosecution of one’s
desires is a different thing from selfishness. The miser is not selfish;
his money does him no good, and he cares for what shall become of it
after his death. We are constantly speaking of _our_ possessions on the
Pacific, and of _our_ destiny as a republic, where no personal interests
are involved, in a way which shows that we have wider ones. We discuss
with anxiety the possible exhaustion of coal in some hundreds of years,
or the cooling-off of the sun in some millions, and show in the most
popular of all religious tenets that we can conceive the possibility of
a man’s descending into hell for the salvation of his fellows.

Now, it is not necessary for logicality that a man should himself be
capable of the heroism of self-sacrifice. It is sufficient that he
should recognize the possibility of it, should perceive that only that
man’s inferences who has it are really logical, and should consequently
regard his own as being only so far valid as they would be accepted by
the hero. So far as he thus refers his inferences to that standard, he
becomes identified with such a mind.

This makes logicality attainable enough. Sometimes we can personally
attain to heroism. The soldier who runs to scale a wall knows that he
will probably be shot, but that is not all he cares for. He also knows
that if all the regiment, with whom in feeling he identifies himself,
rush forward at once, the fort will be taken. In other cases we can only
imitate the virtue. The man whom we have supposed as having to draw from
the two packs, who if he is not a logician will draw from the red pack
from mere habit, will see, if he is logician enough, that he cannot be
logical so long as he is concerned only with his own fate, but that that
man who should care equally for what was to happen in all possible cases
of the sort could act logically, and would draw from the pack with the
most red cards, and thus, though incapable himself of such sublimity,
our logician would imitate the effect of that man’s courage in order to
share his logicality.

But all this requires a conceived identification of one’s interests with
those of an unlimited community. Now, there exist no reasons, and a
later discussion will show that there can be no reasons, for thinking
that the human race, or any intellectual race, will exist forever. On
the other hand, there can be no reason against it;[39] and, fortunately,
as the whole requirement is that we should have certain sentiments,
there is nothing in the facts to forbid our having a _hope_, or calm and
cheerful wish, that the community may last beyond any assignable date.

It may seem strange that I should put forward three sentiments, namely,
interest in an indefinite community, recognition of the possibility of
this interest being made supreme, and hope in the unlimited continuance
of intellectual activity, as indispensable requirements of logic. Yet,
when we consider that logic depends on a mere struggle to escape doubt,
which, as it terminates in action, must begin in emotion, and that,
furthermore, the only cause of our planting ourselves on reason is that
other methods of escaping doubt fail on account of the social impulse,
why should we wonder to find social sentiment presupposed in reasoning?
As for the other two sentiments which I find necessary, they are so only
as supports and accessories of that. It interests me to notice that
these three sentiments seem to be pretty much the same as that famous
trio of Charity, Faith, and Hope, which, in the estimation of St. Paul,
are the finest and greatest of spiritual gifts. Neither Old nor New
Testament is a textbook of the logic of science, but the latter is
certainly the highest existing authority in regard to the dispositions
of heart which a man ought to have.

# V

Such average statistical numbers as the number of inhabitants per square
mile, the average number of deaths per week, the number of convictions
per indictment, or, generally speaking, the numbers of _x_’s per _y_,
where the _x_’s are a class of things some or all of which are connected
with another class of things, their _y_’s, I term _relative numbers_. Of
the two classes of things to which a relative number refers, that one of
which it is a number may be called its _relate_, and that one _per_
which the numeration is made may be called its _correlate_.

Probability is a kind of relative number; namely, it is the ratio of the
number of arguments of a certain genus which carry truth with them to
the total number of arguments of that genus, and the rules for the
calculation of probabilities are very easily derived from this
consideration. They may all be given here, since they are extremely
simple, and it is sometimes convenient to know something of the
elementary rules of calculation of chances.

**RULE I. Direct Calculation.**
To calculate, directly, any relative
number, say for instance the number of passengers in the average trip of
a street-car, we must proceed as follows:

Count the number of passengers for each trip; add all these numbers, and
divide by the number of trips. There are cases in which this rule may be
simplified. Suppose we wish to know the number of inhabitants to a
dwelling in New York. The same person cannot inhabit two dwellings. If
he divide his time between two dwellings he ought to be counted a
half-inhabitant of each. In this case we have only to divide the total
number of the inhabitants of New York by the number of their dwellings,
without the necessity of counting separately those which inhabit each
one. A similar proceeding will apply wherever each individual relate
belongs to one individual correlate exclusively. If we want the number
of _x_’s per _y_, and no _x_ belongs to more than one _y_, we have only
to divide the whole number of _x_’s of _y_’s by the number of _y_’s.
Such a method would, of course, fail if applied to finding the average
number of street-car passengers per trip. We could not divide the total
number of travelers by the number of trips, since many of them would
have made many passages.

To find the probability that from a given class of premises, A, a given
class of conclusions, B, follow, it is simply necessary to ascertain
what proportion of the times in which premises of that class are true,
the appropriate conclusions are also true. In other words, it is the
number of cases of the occurrence of both the events A and B, divided by
the total number of cases of the occurrence of the event A.

**RULE II. Addition of Relative Numbers.**
Given two relative numbers
having the same correlate, say the number of _x_’s per _y_, and the
number of _z_’s per _y_; it is required to find the number of _x_’s and
_z_’s together per _y_. If there is nothing which is at once an _x_ and
a _z_ to the same _y_, the sum of the two given numbers would give the
required number. Suppose, for example, that we had given the average
number of friends that men have, and the average number of enemies, the
sum of these two is the average number of persons interested in a man.
On the other hand, it plainly would not do to add the average number of
persons having constitutional diseases and over military age, to the
average number exempted by each special cause from military service, in
order to get the average number exempt in any way, since many are exempt
in two or more ways at once.

This rule applies directly to probabilities, given the probability that
two different and mutually exclusive events will happen under the same
supposed set of circumstances. Given, for instance, the probability that
if A then B, and also the probability that if A then C, then the sum of
these two probabilities is the probability that if A then either B or C,
so long as there is no event which belongs at once to the two classes B
and C.

**RULE III. Multiplication of Relative Numbers.**
Suppose that we have
given the relative number of _x_’s per _y_; also the relative number of
_z_’s per _x_ of _y_; or, to take a concrete example, suppose that we
have given, first, the average number of children in families living in
New York; and, second, the average number of teeth in the head of a New
York child—then the product of these two numbers would give the average
number of children’s teeth in a New York family. But this mode of
reckoning will only apply in general under two restrictions. In the
first place, it would not be true if the same child could belong to
different families, for in that case those children who belonged to
several different families might have an exceptionally large or small
number of teeth, which would affect the average number of children’s
teeth in a family more than it would affect the average number of teeth
in a child’s head. In the second place, the rule would not be true if
different children could share the same teeth, the average number of
children’s teeth being in that case evidently something different from
the average number of teeth belonging to a child.

- **Xie:**
  The following conditional probability equation:

  ```
  P(Z, X | Y) = P(Z | Y, X) * P(X | Y)
  ```

  is much like the composition of functions:

  ```
  P(Y -> Z, X) = P(Y, X -> Z) * P(Y -> X)
  ```

  Note that the following equation does not hold in general:

  ```
  P(A -> C) = P(A -> B) * P(B -> C)
  ```

  because

  ```
  P(A, C) / P(A) !=
  P(A, B) / P(A) *
  P(B, C) / P(B)
  ```

  unless `A, B, C` are mutually independent.

In order to apply this rule to probabilities, we must proceed as
follows: Suppose that we have given the probability that the conclusion
B follows from the premise A, B and A representing as usual certain
classes of propositions. Suppose that we also knew the probability of an
inference in which B should be the premise, and a proposition of a third
kind, C, the conclusion. Here, then, we have the materials for the
application of this rule. We have, first, the relative number of B’s per
A. We next should have the relative number of C’s per B following from
A. But the classes of propositions being so selected that the
probability of C following from any B in general is just the same as the
probability of C’s following from one of those B’s which is deducible
from an A, the two probabilities may be multiplied together, in order to
give the probability of C following from A. The same restrictions exist
as before. It might happen that the probability that B follows from A
was affected by certain propositions of the class B following from
several different propositions of the class A. But, practically
speaking, all these restrictions are of very little consequence, and it
is usually recognized as a principle universally true that the
probability that, if A is true, B is, multiplied by the probability
that, if B is true, C is, gives the probability that, if A is true, C
is.

There is a rule supplementary to this, of which great use is made. It is
not universally valid, and the greatest caution has to be exercised in
making use of it—a double care, first, never to use it when it will
involve serious error; and, second, never to fail to take advantage of
it in cases in which it can be employed. This rule depends upon the fact
that in very many cases the probability that C is true if B is, is
substantially the same as the probability that C is true if A is.
Suppose, for example, we have the average number of males among the
children born in New York; suppose that we also have the average number
of children born in the winter months among those born in New York. Now,
we may assume without doubt, at least as a closely approximate
proposition (and no very nice calculation would be in place in regard to
probabilities), that the proportion of males among all the children born
in New York is the same as the proportion of males born in summer in New
York; and, therefore, if the names of all the children born during a
year were put into an urn, we might multiply the probability that any
name drawn would be the name of a male child by the probability that it
would be the name of a child born in summer, in order to obtain the
probability that it would be the name of a male child born in summer.
The questions of probability, in the treatises upon the subject, have
usually been such as relate to balls drawn from urns, and games of
cards, and so on, in which the question of the _independence_ of events,
as it is called—that is to say, the question of whether the probability
of C, under the hypothesis B, is the same as its probability under the
hypothesis A, has been very simple; but, in the application of
probabilities to the ordinary questions of life, it is often an
exceedingly nice question whether two events may be considered as
independent with sufficient accuracy or not. In all calculations about
cards it is assumed that the cards are thoroughly shuffled, which makes
one deal quite independent of another. In point of fact the cards seldom
are, in practice, shuffled sufficiently to make this true; thus, in a
game of whist, in which the cards have fallen in suits of four of the
same suit, and are so gathered up, they will lie more or less in sets of
four of the same suit, and this will be true even after they are
shuffled. At least some traces of this arrangement will remain, in
consequence of which the number of “short suits,” as they are
called—that is to say, the number of hands in which the cards are very
unequally divided in regard to suits—is smaller than the calculation
would make it to be; so that, when there is a misdeal, where the cards,
being thrown about the table, get very thoroughly shuffled, it is a
common saying that in the hands next dealt out there are generally short
suits. A few years ago a friend of mine, who plays whist a great deal,
was so good as to count the number of spades dealt to him in 165 hands,
in which the cards had been, if anything, shuffled better than usual.
According to calculation, there should have been 85 of these hands in
which my friend held either three or four spades, but in point of fact
there were 94, showing the influence of imperfect shuffling.

According to the view here taken, these are the only fundamental rules
for the calculation of chances. An additional one, derived from a
different conception of probability, is given in some treatises, which
if it be sound might be made the basis of a theory of reasoning. Being,
as I believe it is, absolutely absurd, the consideration of it serves to
bring us to the true theory; and it is for the sake of this discussion,
which must be postponed to the next number, that I have brought the
doctrine of chances to the reader’s attention at this early stage of our
studies of the logic of science.

# Footnotes

- Footnote 34:

  _Popular Science Monthly_, March, 1878.

- Footnote 35:

  [Later, pp. 170 ff. and 215 ff., it is shown that continuity is also
  at the basis of mathematical generalization. See also article on
  Synechism in _Baldwin’s Dictionary of Philosophy_.]

- Footnote 36:

  This mode of thought is so familiarly associated with all exact
  numerical consideration, that the phrase appropriate to it is imitated
  by shallow writers in order to produce the appearance of exactitude
  where none exists. Certain newspapers which affect a learned tone talk
  of “the average man,” when they simply mean _most men_, and have no
  idea of striking an average.

- Footnote 37:

  _Cf._ pp. 179 ff. below.

- Footnote 38:

  The conception of probability here set forth is substantially that
  first developed by Mr. Venn, in his _Logic of Chance_. Of course, a
  vague apprehension of the idea had always existed, but the problem was
  to make it perfectly clear, and to him belongs the credit of first
  doing this.

- Footnote 39:

  I do not here admit an absolutely unknowable. Evidence could show us
  what would probably be the case after any given lapse of time; and
  though a subsequent time might be assigned which that evidence might
  not cover, yet further evidence would cover it.
